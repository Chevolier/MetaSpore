nohup: ignoring input
Debug -- Ctr Demo Wide&Deep
Debug--load config: {'app_name': 'WideDeep Demo', 'local': False, 'worker_count': 2, 'server_count': 2, 'batch_size': 1, 'worker_memory': '10G', 'server_memory': '10G', 'coordinator_memory': '30G', 'train_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/1m/rank/train.parquet', 'test_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/1m/rank/test.parquet', 'column_name_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/column_schema', 'combine_schema_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/combine_column_schema', 'wide_combine_schema_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/wide_combine_column_schema', 'model_in_path': None, 'model_out_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/model/widedeep/model_out/ml_1m/', 'model_export_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/model/widedeep/model_export/ml_1m/', 'model_version': '0.1', 'experiment_name': 'movie_lens_wdl_1m', 'input_label_column_index': 0, 'embedding_size': 10, 'net_dropout': 0.0, 'adam_learning_rate': 1e-05, 'ftrl_alpha': 0.02, 'ftrl_beta': 1.0, 'ftrl_l1': 1.0, 'ftrl_l2': 1.0, 'dnn_hidden_units': [1024, 512, 256, 128]}
Before update: {'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7f7c820baf10>, '__spec__': None, '__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__file__': 'widedeep.py', '__cached__': None, 'ms': <module 'metaspore' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/__init__.py'>, 'pyspark': <module 'pyspark' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyspark/__init__.py'>, 'np': <module 'numpy' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/numpy/__init__.py'>, 'yaml': <module 'yaml' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/yaml/__init__.py'>, 'subprocess': <module 'subprocess' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/subprocess.py'>, 'argparse': <module 'argparse' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/argparse.py'>, 'sys': <module 'sys' (built-in)>, 'itemgetter': <class 'operator.itemgetter'>, 'WideDeep': <class 'python.algos.widedeep_net.WideDeep'>, 'load_config': <function load_config at 0x7f7c820c4280>, 'init_spark': <function init_spark at 0x7f7c514af940>, 'stop_spark': <function stop_spark at 0x7f7c514c4a60>, 'read_dataset': <function read_dataset at 0x7f7c514c4af0>, 'train': <function train at 0x7f7c514c4b80>, 'transform': <function transform at 0x7f7c514c4c10>, 'evaluate': <function evaluate at 0x7f7c514c4ca0>, 'parser': ArgumentParser(prog='widedeep.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True), 'args': Namespace(conf='conf/widedeep_ml_1m.yaml.dev'), 'params': {'app_name': 'WideDeep Demo', 'local': False, 'worker_count': 2, 'server_count': 2, 'batch_size': 1, 'worker_memory': '10G', 'server_memory': '10G', 'coordinator_memory': '30G', 'train_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/1m/rank/train.parquet', 'test_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/1m/rank/test.parquet', 'column_name_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/column_schema', 'combine_schema_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/combine_column_schema', 'wide_combine_schema_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/wide_combine_column_schema', 'model_in_path': None, 'model_out_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/model/widedeep/model_out/ml_1m/', 'model_export_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/model/widedeep/model_export/ml_1m/', 'model_version': '0.1', 'experiment_name': 'movie_lens_wdl_1m', 'input_label_column_index': 0, 'embedding_size': 10, 'net_dropout': 0.0, 'adam_learning_rate': 1e-05, 'ftrl_alpha': 0.02, 'ftrl_beta': 1.0, 'ftrl_l1': 1.0, 'ftrl_l2': 1.0, 'dnn_hidden_units': [1024, 512, 256, 128]}}
After update: updating: python/ (stored 0%)
updating: python/docker_build_all_wheels.py (deflated 66%)
updating: python/algos/ (stored 0%)
updating: python/algos/item_cf_retrieval.py (deflated 77%)
updating: python/algos/pipeline/ (stored 0%)
updating: python/algos/pipeline/mongodb_dumper.py (deflated 61%)
updating: python/algos/pipeline/utils/ (stored 0%)
updating: python/algos/pipeline/utils/constants.py (deflated 42%)
updating: python/algos/pipeline/utils/logger.py (deflated 48%)
updating: python/algos/pipeline/utils/__init__.py (deflated 38%)
updating: python/algos/pipeline/utils/class_utils.py (deflated 52%)
updating: python/algos/pipeline/utils/dict_utils.py (deflated 39%)
updating: python/algos/pipeline/common_validators.py (deflated 66%)
updating: python/algos/pipeline/__init__.py (deflated 52%)
updating: python/algos/pipeline/i2i_retrieval.py (deflated 75%)
updating: python/algos/pipeline/data_loader.py (deflated 66%)
updating: python/algos/pipeline/init_spark.py (deflated 61%)
updating: python/algos/pipeline/deep_ctr.py (deflated 77%)
updating: python/algos/pipeline/popular_retrieval.py (deflated 67%)
updating: python/algos/layers.py (deflated 79%)
updating: python/algos/pnn_net.py (deflated 70%)
updating: python/algos/twotower/ (stored 0%)
updating: python/algos/twotower/dssm/ (stored 0%)
updating: python/algos/twotower/dssm/__init__.py (deflated 41%)
updating: python/algos/twotower/dssm/dssm_agent.py (deflated 72%)
updating: python/algos/twotower/dssm/dssm_net.py (deflated 78%)
updating: python/algos/twotower/__init__.py (deflated 37%)
updating: python/algos/twotower/simplex/ (stored 0%)
updating: python/algos/twotower/simplex/__init__.py (deflated 39%)
updating: python/algos/twotower/simplex/simplex_agent.py (deflated 64%)
updating: python/algos/twotower/simplex/simplex_net.py (deflated 71%)
updating: python/algos/dcn_v2_net.py (deflated 75%)
updating: python/algos/__init__.py (deflated 37%)
updating: python/algos/widedeep_net.py (deflated 68%)
updating: python/algos/feature/ (stored 0%)
updating: python/algos/feature/neg_sampler.py (deflated 66%)
updating: python/algos/feature/__init__.py (deflated 41%)
updating: python/algos/feature/target_encoder.py (deflated 51%)
updating: python/algos/feature/sequential_encoder.py (deflated 60%)
updating: python/algos/feature/woe_encoder.py (deflated 64%)
updating: python/algos/ffm_net.py (deflated 71%)
updating: python/algos/tuner/ (stored 0%)
updating: python/algos/tuner/base_tuner.py (deflated 70%)
updating: python/algos/autoint_net.py (deflated 70%)
updating: python/algos/sequential/ (stored 0%)
updating: python/algos/sequential/__init__.py (deflated 49%)
updating: python/algos/sequential/dien/ (stored 0%)
updating: python/algos/sequential/dien/dien_agent.py (deflated 61%)
updating: python/algos/sequential/dien/dien_net.py (deflated 79%)
updating: python/algos/sequential/gru4rec/ (stored 0%)
updating: python/algos/sequential/gru4rec/gru4rec_net.py (deflated 74%)
updating: python/algos/sequential/gru4rec/gru4rec_agent.py (deflated 65%)
updating: python/algos/sequential/bst/ (stored 0%)
updating: python/algos/sequential/bst/bst_net.py (deflated 77%)
updating: python/algos/sequential/hrm/ (stored 0%)
updating: python/algos/sequential/hrm/hrm_net.py (deflated 77%)
updating: python/algos/sequential/din/ (stored 0%)
updating: python/algos/sequential/din/din_net.py (deflated 77%)
updating: python/algos/xdeepfm_net.py (deflated 71%)
updating: python/algos/graph/ (stored 0%)
updating: python/algos/graph/jaccard_retrieval.py (deflated 74%)
updating: python/algos/graph/__init__.py (deflated 43%)
updating: python/algos/graph/node2vec_retrieval.py (deflated 78%)
updating: python/algos/graph/euclidean_retrieval.py (deflated 74%)
updating: python/algos/multitask/ (stored 0%)
updating: python/algos/multitask/__init__.py (deflated 43%)
updating: python/algos/multitask/esmm/ (stored 0%)
updating: python/algos/multitask/esmm/esmm_net.py (deflated 69%)
updating: python/algos/multitask/esmm/esmm_agent.py (deflated 74%)
updating: python/algos/multitask/mmoe/ (stored 0%)
updating: python/algos/multitask/mmoe/mmoe_agent.py (deflated 70%)
updating: python/algos/multitask/mmoe/mmoe_net.py (deflated 75%)
updating: python/algos/dcn_net.py (deflated 69%)
updating: python/algos/fwfm_net.py (deflated 69%)
updating: python/algos/__pycache__/ (stored 0%)
updating: python/algos/__pycache__/widedeep_net.cpython-38.pyc (deflated 41%)
updating: python/algos/__pycache__/__init__.cpython-38.pyc (deflated 21%)
updating: python/algos/__pycache__/layers.cpython-38.pyc (deflated 60%)
updating: python/algos/deepfm_net.py (deflated 69%)
updating: python/__init__.py (stored 0%)
updating: python/metasporecli/ (stored 0%)
updating: python/metasporecli/python/ (stored 0%)
updating: python/metasporecli/python/metasporecli/ (stored 0%)
updating: python/metasporecli/python/metasporecli/__main__.py (deflated 37%)
updating: python/metasporecli/python/metasporecli/__init__.py (deflated 37%)
updating: python/metasporecli/python/metasporecli/flow.py (deflated 71%)
updating: python/metasporecli/python/metasporecli/main.py (deflated 59%)
updating: python/metasporecli/README.md (deflated 61%)
updating: python/metasporecli/.gitignore (stored 0%)
updating: python/metasporecli/pyproject.toml (deflated 45%)
updating: python/scripts/ (stored 0%)
updating: python/scripts/consul/ (stored 0%)
updating: python/scripts/consul/consul_watch_load.py (deflated 72%)
updating: python/scripts/consul/Dockerfile (deflated 44%)
updating: python/scripts/consul/build.sh (deflated 30%)
updating: python/scripts/consul/create_consul_watch.sh (deflated 17%)
updating: python/scripts/preprocessing/ (stored 0%)
updating: python/scripts/preprocessing/example_preprocessor.py (deflated 54%)
updating: python/scripts/preprocessing/test_example_preprocessor.py (deflated 51%)
updating: python/scripts/preprocessing/example_requirements.txt (stored 0%)
updating: python/scripts/preprocessing/preprocessor_service.py (deflated 69%)
updating: python/ps/ (stored 0%)
updating: python/ps/__init__.py (deflated 42%)
updating: python/ps/job.py (deflated 81%)
updating: python/metasporeflow/ (stored 0%)
updating: python/metasporeflow/python/ (stored 0%)
updating: python/metasporeflow/python/__init__.py (stored 0%)
updating: python/metasporeflow/python/metasporeflow/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/executors/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/executors/flow_executor_factory.py (deflated 60%)
updating: python/metasporeflow/python/metasporeflow/executors/k8s_cluster_flow_executor.py (deflated 73%)
updating: python/metasporeflow/python/metasporeflow/executors/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/executors/sage_maker_flow_executor.py (deflated 69%)
updating: python/metasporeflow/python/metasporeflow/executors/flow_executor.py (deflated 50%)
updating: python/metasporeflow/python/metasporeflow/executors/local_flow_executor.py (deflated 67%)
updating: python/metasporeflow/python/metasporeflow/flows/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/flows/flow_loader.py (deflated 66%)
updating: python/metasporeflow/python/metasporeflow/flows/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/flows/aws_tracking_config.py (deflated 49%)
updating: python/metasporeflow/python/metasporeflow/flows/metaspore_offline_flow.py (deflated 55%)
updating: python/metasporeflow/python/metasporeflow/flows/metaspore_flow.py (deflated 39%)
updating: python/metasporeflow/python/metasporeflow/flows/sage_maker_config.py (deflated 40%)
updating: python/metasporeflow/python/metasporeflow/tracking/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/tracking.py (deflated 50%)
updating: python/metasporeflow/python/metasporeflow/tracking/__init__.py (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/README.md (deflated 70%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/Dockerfile (deflated 41%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/extensionssrc/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/extensionssrc/requirements.txt (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/extensionssrc/extensions/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/extensionssrc/extensions/logs_api_http_extension.py (deflated 64%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/extensionssrc/extensions/logs_api_http_extension/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/extensionssrc/extensions/logs_api_http_extension/extensions_api_client.py (deflated 67%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/extensionssrc/extensions/logs_api_http_extension/__init__.py (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/extensionssrc/extensions/logs_api_http_extension/logs_api_client.py (deflated 58%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/extension/extensionssrc/extensions/logs_api_http_extension/http_listener.py (deflated 55%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/function/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/function/functionsrc/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/function/functionsrc/requirements.txt (stored 0%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/function/functionsrc/Dockerfile (deflated 23%)
updating: python/metasporeflow/python/metasporeflow/tracking/aws/function/functionsrc/app.py (deflated 51%)
updating: python/metasporeflow/python/metasporeflow/tracking/tracking_aws.py (deflated 74%)
updating: python/metasporeflow/python/metasporeflow/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/online/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/online/compose_config.py (deflated 73%)
updating: python/metasporeflow/python/metasporeflow/online/online_flow.py (deflated 70%)
updating: python/metasporeflow/python/metasporeflow/online/online_executor.py (deflated 76%)
updating: python/metasporeflow/python/metasporeflow/online/k8s_template/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/online/k8s_template/model_template.py (deflated 68%)
updating: python/metasporeflow/python/metasporeflow/online/k8s_template/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/online/k8s_template/recommend_template.py (deflated 63%)
updating: python/metasporeflow/python/metasporeflow/online/k8s_template/consul_template.py (deflated 68%)
updating: python/metasporeflow/python/metasporeflow/online/online_k8s_executor.py (deflated 80%)
updating: python/metasporeflow/python/metasporeflow/online/monitor_service.py (deflated 61%)
updating: python/metasporeflow/python/metasporeflow/online/common.py (deflated 61%)
updating: python/metasporeflow/python/metasporeflow/online/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/online/sagemaker_executor.py (deflated 79%)
updating: python/metasporeflow/python/metasporeflow/online/cloud_consul.py (deflated 56%)
updating: python/metasporeflow/python/metasporeflow/online/sagemaker/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/online/sagemaker/Dockerfile (deflated 64%)
updating: python/metasporeflow/python/metasporeflow/online/sagemaker/dockerd-entrypoint.py (deflated 66%)
updating: python/metasporeflow/python/metasporeflow/online/README.md (deflated 60%)
updating: python/metasporeflow/python/metasporeflow/online/test/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/online/test/online_local_flow.yml (deflated 72%)
updating: python/metasporeflow/python/metasporeflow/online/test/metaspore-flow.yml (deflated 23%)
updating: python/metasporeflow/python/metasporeflow/online/benchmark_wrk.lua (deflated 43%)
updating: python/metasporeflow/python/metasporeflow/online/online_generator.py (deflated 83%)
updating: python/metasporeflow/python/metasporeflow/online/script/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/online/script/run_recommend_serving.sh (deflated 66%)
updating: python/metasporeflow/python/metasporeflow/online/check_service.py (deflated 70%)
updating: python/metasporeflow/python/metasporeflow/online/service_config.py (deflated 82%)
updating: python/metasporeflow/python/metasporeflow/runners/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/runners/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/runners/crontab_sage_maker_runner.py (deflated 71%)
updating: python/metasporeflow/python/metasporeflow/offline/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/offline/scheduler/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/offline/scheduler/offline_crontab_scheduler.py (deflated 70%)
updating: python/metasporeflow/python/metasporeflow/offline/scheduler/scheduler_type.py (deflated 45%)
updating: python/metasporeflow/python/metasporeflow/offline/scheduler/k8s_job_config_generator.py (deflated 76%)
updating: python/metasporeflow/python/metasporeflow/offline/scheduler/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/offline/scheduler/offline_k8s_cronjob_scheduler.py (deflated 71%)
updating: python/metasporeflow/python/metasporeflow/offline/scheduler/scheduler.py (deflated 56%)
updating: python/metasporeflow/python/metasporeflow/offline/scheduler/sage_maker_entrypoint_generator.py (deflated 51%)
updating: python/metasporeflow/python/metasporeflow/offline/scheduler/offline_sage_maker_scheduler.py (deflated 76%)
updating: python/metasporeflow/python/metasporeflow/offline/utils/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/offline/utils/file_util.py (deflated 54%)
updating: python/metasporeflow/python/metasporeflow/offline/utils/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/offline/scheduler_manager.py (deflated 70%)
updating: python/metasporeflow/python/metasporeflow/offline/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/offline/k8s_cluster_offline_executor.py (deflated 56%)
updating: python/metasporeflow/python/metasporeflow/offline/task/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/offline/task/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/offline/task/offline_python_task.py (deflated 50%)
updating: python/metasporeflow/python/metasporeflow/offline/task/task.py (deflated 57%)
updating: python/metasporeflow/python/metasporeflow/offline/sage_maker_offline_executor.py (deflated 58%)
updating: python/metasporeflow/python/metasporeflow/offline/task_manager.py (deflated 59%)
updating: python/metasporeflow/python/metasporeflow/offline/local_offline_executor.py (deflated 67%)
updating: python/metasporeflow/python/metasporeflow/resources/ (stored 0%)
updating: python/metasporeflow/python/metasporeflow/resources/resource.py (deflated 39%)
updating: python/metasporeflow/python/metasporeflow/resources/__init__.py (deflated 37%)
updating: python/metasporeflow/python/metasporeflow/resources/resource_manager.py (deflated 69%)
updating: python/metasporeflow/python/metasporeflow/resources/resource_loader.py (deflated 73%)
updating: python/metasporeflow/README.md (deflated 6%)
updating: python/metasporeflow/.gitignore (deflated 14%)
updating: python/metasporeflow/pyproject.toml (deflated 45%)
updating: python/metaspore/ (stored 0%)
updating: python/metaspore/spark.py (deflated 71%)
updating: python/metaspore/cast.py (deflated 60%)
updating: python/metaspore/demo.py (deflated 54%)
updating: python/metaspore/initializer.py (deflated 76%)
updating: python/metaspore/s3_utils.py (deflated 73%)
updating: python/metaspore/compat/ (stored 0%)
updating: python/metaspore/compat/__init__.py (deflated 52%)
updating: python/metaspore/compat/ps/ (stored 0%)
updating: python/metaspore/compat/ps/__init__.py (deflated 66%)
updating: python/metaspore/algos/ (stored 0%)
updating: python/metaspore/algos/item_cf_retrieval.py (deflated 77%)
updating: python/metaspore/algos/pipeline/ (stored 0%)
updating: python/metaspore/algos/pipeline/mongodb_dumper.py (deflated 61%)
updating: python/metaspore/algos/pipeline/utils/ (stored 0%)
updating: python/metaspore/algos/pipeline/utils/constants.py (deflated 42%)
updating: python/metaspore/algos/pipeline/utils/logger.py (deflated 48%)
updating: python/metaspore/algos/pipeline/utils/__init__.py (deflated 38%)
updating: python/metaspore/algos/pipeline/utils/class_utils.py (deflated 52%)
updating: python/metaspore/algos/pipeline/utils/dict_utils.py (deflated 39%)
updating: python/metaspore/algos/pipeline/common_validators.py (deflated 66%)
updating: python/metaspore/algos/pipeline/__init__.py (deflated 52%)
updating: python/metaspore/algos/pipeline/i2i_retrieval.py (deflated 75%)
updating: python/metaspore/algos/pipeline/data_loader.py (deflated 66%)
updating: python/metaspore/algos/pipeline/init_spark.py (deflated 61%)
updating: python/metaspore/algos/pipeline/deep_ctr.py (deflated 77%)
updating: python/metaspore/algos/pipeline/popular_retrieval.py (deflated 67%)
updating: python/metaspore/algos/layers.py (deflated 79%)
updating: python/metaspore/algos/pnn_net.py (deflated 70%)
updating: python/metaspore/algos/twotower/ (stored 0%)
updating: python/metaspore/algos/twotower/dssm/ (stored 0%)
updating: python/metaspore/algos/twotower/dssm/__init__.py (deflated 41%)
updating: python/metaspore/algos/twotower/dssm/dssm_agent.py (deflated 72%)
updating: python/metaspore/algos/twotower/dssm/dssm_net.py (deflated 78%)
updating: python/metaspore/algos/twotower/__init__.py (deflated 37%)
updating: python/metaspore/algos/twotower/simplex/ (stored 0%)
updating: python/metaspore/algos/twotower/simplex/__init__.py (deflated 39%)
updating: python/metaspore/algos/twotower/simplex/simplex_agent.py (deflated 64%)
updating: python/metaspore/algos/twotower/simplex/simplex_net.py (deflated 71%)
updating: python/metaspore/algos/dcn_v2_net.py (deflated 75%)
updating: python/metaspore/algos/__init__.py (deflated 37%)
updating: python/metaspore/algos/widedeep_net.py (deflated 68%)
updating: python/metaspore/algos/feature/ (stored 0%)
updating: python/metaspore/algos/feature/neg_sampler.py (deflated 66%)
updating: python/metaspore/algos/feature/__init__.py (deflated 41%)
updating: python/metaspore/algos/feature/target_encoder.py (deflated 51%)
updating: python/metaspore/algos/feature/sequential_encoder.py (deflated 60%)
updating: python/metaspore/algos/feature/woe_encoder.py (deflated 64%)
updating: python/metaspore/algos/ffm_net.py (deflated 71%)
updating: python/metaspore/algos/tuner/ (stored 0%)
updating: python/metaspore/algos/tuner/base_tuner.py (deflated 70%)
updating: python/metaspore/algos/autoint_net.py (deflated 70%)
updating: python/metaspore/algos/sequential/ (stored 0%)
updating: python/metaspore/algos/sequential/__init__.py (deflated 49%)
updating: python/metaspore/algos/sequential/dien/ (stored 0%)
updating: python/metaspore/algos/sequential/dien/dien_agent.py (deflated 61%)
updating: python/metaspore/algos/sequential/dien/dien_net.py (deflated 79%)
updating: python/metaspore/algos/sequential/gru4rec/ (stored 0%)
updating: python/metaspore/algos/sequential/gru4rec/gru4rec_net.py (deflated 74%)
updating: python/metaspore/algos/sequential/gru4rec/gru4rec_agent.py (deflated 65%)
updating: python/metaspore/algos/sequential/bst/ (stored 0%)
updating: python/metaspore/algos/sequential/bst/bst_net.py (deflated 77%)
updating: python/metaspore/algos/sequential/hrm/ (stored 0%)
updating: python/metaspore/algos/sequential/hrm/hrm_net.py (deflated 77%)
updating: python/metaspore/algos/sequential/din/ (stored 0%)
updating: python/metaspore/algos/sequential/din/din_net.py (deflated 77%)
updating: python/metaspore/algos/xdeepfm_net.py (deflated 71%)
updating: python/metaspore/algos/graph/ (stored 0%)
updating: python/metaspore/algos/graph/jaccard_retrieval.py (deflated 74%)
updating: python/metaspore/algos/graph/__init__.py (deflated 43%)
updating: python/metaspore/algos/graph/node2vec_retrieval.py (deflated 78%)
updating: python/metaspore/algos/graph/euclidean_retrieval.py (deflated 74%)
updating: python/metaspore/algos/multitask/ (stored 0%)
updating: python/metaspore/algos/multitask/__init__.py (deflated 43%)
updating: python/metaspore/algos/multitask/esmm/ (stored 0%)
updating: python/metaspore/algos/multitask/esmm/esmm_net.py (deflated 69%)
updating: python/metaspore/algos/multitask/esmm/esmm_agent.py (deflated 74%)
updating: python/metaspore/algos/multitask/mmoe/ (stored 0%)
updating: python/metaspore/algos/multitask/mmoe/mmoe_agent.py (deflated 70%)
updating: python/metaspore/algos/multitask/mmoe/mmoe_net.py (deflated 75%)
updating: python/metaspore/algos/dcn_net.py (deflated 69%)
updating: python/metaspore/algos/fwfm_net.py (deflated 69%)
updating: python/metaspore/algos/__pycache__/ (stored 0%)
updating: python/metaspore/algos/__pycache__/widedeep_net.cpython-38.pyc (deflated 41%)
updating: python/metaspore/algos/__pycache__/__init__.cpython-38.pyc (deflated 21%)
updating: python/metaspore/algos/__pycache__/layers.cpython-38.pyc (deflated 60%)
updating: python/metaspore/algos/deepfm_net.py (deflated 69%)
updating: python/metaspore/metric.py (deflated 78%)
updating: python/metaspore/__init__.py (deflated 70%)
updating: python/metaspore/embedding.py (deflated 81%)
updating: python/metaspore/input.py (deflated 59%)
updating: python/metaspore/url_utils.py (deflated 51%)
updating: python/metaspore/feature_group.py (deflated 58%)
updating: python/metaspore/schema_utils.py (deflated 70%)
updating: python/metaspore/experiment.py (deflated 76%)
updating: python/metaspore/estimator.py (deflated 85%)
updating: python/metaspore/nn/ (stored 0%)
updating: python/metaspore/nn/fm.py (deflated 47%)
updating: python/metaspore/nn/__init__.py (deflated 41%)
updating: python/metaspore/nn/normalization.py (deflated 61%)
updating: python/metaspore/nn/deep_fm.py (deflated 70%)
updating: python/metaspore/nn/wide_and_deep.py (deflated 75%)
updating: python/metaspore/swing_retrieval.py (deflated 79%)
updating: python/metaspore/two_tower_retrieval.py (deflated 85%)
updating: python/metaspore/name_utils.py (deflated 51%)
updating: python/metaspore/stack_trace_utils.py (deflated 43%)
updating: python/metaspore/network_utils.py (deflated 52%)
updating: python/metaspore/agent.py (deflated 77%)
updating: python/metaspore/job_utils.py (deflated 47%)
updating: python/metaspore/ps_launcher.py (deflated 68%)
updating: python/metaspore/two_tower_ranking.py (deflated 80%)
updating: python/metaspore/distributed_tensor.py (deflated 80%)
updating: python/metaspore/updater.py (deflated 81%)
updating: python/metaspore/distributed_trainer.py (deflated 76%)
updating: python/metaspore/file_utils.py (deflated 69%)
updating: python/metaspore/loss_utils.py (deflated 40%)
updating: python/metaspore/shell_utils.py (deflated 65%)
updating: python/metaspore/model.py (deflated 81%)
updating: python/metaspore/patching_pickle.py (deflated 72%)
updating: python/metaspore/output.py (deflated 56%)
updating: python/__pycache__/ (stored 0%)
updating: python/__pycache__/__init__.cpython-38.pyc (deflated 23%)
updating: python/tests/ (stored 0%)
updating: python/tests/dense_xgboost_grpc_test.py (deflated 44%)
updating: python/tests/requirements.txt (deflated 24%)
updating: python/tests/verify_java_tensor_serde.py (deflated 42%)
updating: python/tests/mnist_mlp.py (deflated 59%)
updating: python/tests/sparse_wdl_grpc_test.py (deflated 52%)
updating: python/tests/sparse_two_tower_export_demo.py (deflated 73%)
updating: python/tests/mnist_mlp_eval.py (deflated 57%)
updating: python/tests/sparse_wdl_export_demo.py (deflated 64%)
updating: python/tests/sparse_mlp_export_demo.py (deflated 64%)
updating: python/tests/two_tower_retrieval_milvus.py (deflated 81%)
updating: python/tests/sparse_wdl_export_test.py (deflated 66%)
updating: python/tests/embedding_bag_export.py (deflated 61%)
updating: python/tests/dense_xgboost.py (deflated 49%)
updating: python/algos/__pycache__/widedeep_net.cpython-39.pyc (deflated 41%)
updating: python/algos/__pycache__/__init__.cpython-39.pyc (deflated 21%)
updating: python/algos/__pycache__/layers.cpython-39.pyc (deflated 60%)
updating: python/metaspore/algos/__pycache__/widedeep_net.cpython-39.pyc (deflated 41%)
updating: python/metaspore/algos/__pycache__/__init__.cpython-39.pyc (deflated 21%)
updating: python/metaspore/algos/__pycache__/layers.cpython-39.pyc (deflated 60%)
updating: python/__pycache__/__init__.cpython-39.pyc (deflated 23%)
updating: python/metaspore/.ipynb_checkpoints/ (stored 0%)
updating: python/metaspore/.ipynb_checkpoints/ps_launcher-checkpoint.py (deflated 68%)
24/07/08 05:37:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/07/08 05:37:35 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/07/08 05:37:36 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[Stage 0:>                                                                                                                                                                                         (0 + 1) / 1]                                                                                                                                                                                                               {'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7f7c820baf10>, '__spec__': None, '__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__file__': 'widedeep.py', '__cached__': None, 'ms': <module 'metaspore' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/__init__.py'>, 'pyspark': <module 'pyspark' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyspark/__init__.py'>, 'np': <module 'numpy' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/numpy/__init__.py'>, 'yaml': <module 'yaml' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/yaml/__init__.py'>, 'subprocess': <module 'subprocess' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/subprocess.py'>, 'argparse': <module 'argparse' from '/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/argparse.py'>, 'sys': <module 'sys' (built-in)>, 'itemgetter': <class 'operator.itemgetter'>, 'WideDeep': <class 'python.algos.widedeep_net.WideDeep'>, 'load_config': <function load_config at 0x7f7c820c4280>, 'init_spark': <function init_spark at 0x7f7c514af940>, 'stop_spark': <function stop_spark at 0x7f7c514c4a60>, 'read_dataset': <function read_dataset at 0x7f7c514c4af0>, 'train': <function train at 0x7f7c514c4b80>, 'transform': <function transform at 0x7f7c514c4c10>, 'evaluate': <function evaluate at 0x7f7c514c4ca0>, 'parser': ArgumentParser(prog='widedeep.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True), 'args': Namespace(conf='conf/widedeep_ml_1m.yaml.dev'), 'params': {'app_name': 'WideDeep Demo', 'local': False, 'worker_count': 2, 'server_count': 2, 'batch_size': 1, 'worker_memory': '10G', 'server_memory': '10G', 'coordinator_memory': '30G', 'train_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/1m/rank/train.parquet', 'test_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/1m/rank/test.parquet', 'column_name_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/column_schema', 'combine_schema_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/combine_column_schema', 'wide_combine_schema_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/wide_combine_column_schema', 'model_in_path': None, 'model_out_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/model/widedeep/model_out/ml_1m/', 'model_export_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/model/widedeep/model_export/ml_1m/', 'model_version': '0.1', 'experiment_name': 'movie_lens_wdl_1m', 'input_label_column_index': 0, 'embedding_size': 10, 'net_dropout': 0.0, 'adam_learning_rate': 1e-05, 'ftrl_alpha': 0.02, 'ftrl_beta': 1.0, 'ftrl_l1': 1.0, 'ftrl_l2': 1.0, 'dnn_hidden_units': [1024, 512, 256, 128]}, 'app_name': 'WideDeep Demo', 'local': False, 'worker_count': 2, 'server_count': 2, 'batch_size': 1, 'worker_memory': '10G', 'server_memory': '10G', 'coordinator_memory': '30G', 'train_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/1m/rank/train.parquet', 'test_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/1m/rank/test.parquet', 'column_name_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/column_schema', 'combine_schema_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/combine_column_schema', 'wide_combine_schema_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/wide_combine_column_schema', 'model_in_path': None, 'model_out_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/model/widedeep/model_out/ml_1m/', 'model_export_path': 's3a://sagemaker-us-west-2-452145973879/datasets/movielens/model/widedeep/model_export/ml_1m/', 'model_version': '0.1', 'experiment_name': 'movie_lens_wdl_1m', 'input_label_column_index': 0, 'embedding_size': 10, 'net_dropout': 0.0, 'adam_learning_rate': 1e-05, 'ftrl_alpha': 0.02, 'ftrl_beta': 1.0, 'ftrl_l1': 1.0, 'ftrl_l2': 1.0, 'dnn_hidden_units': [1024, 512, 256, 128]}
Debug -- spark init
Debug -- version: 3.3.4
Debug -- applicaitonId: local-1720417055870
Debug -- uiWebUrl: http://ip-172-16-14-249.us-west-2.compute.internal:4040
Debug -- match train dataset sample:
+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+--------------------+----+--------------------+----------+------------------+
|label|user_id|gender|age|occupation|  zip|movie_id|    recent_movie_ids| recent_movie_genres|  recent_movie_years|year|               genre|last_movie|        last_genre|
+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+--------------------+----+--------------------+----------+------------------+
|    1|    683|     M| 25|         4|27514|     733|88155625712028...|ComedyAction|Rom...|19961997199919...|1996|ActionAdventure...|      1610|   ActionThriller|
|    1|   4779|     M| 45|         3|04011|     910|11781219912362...|Drama|WarHorror|...|19571960194219...|1959|        ComedyCrime|      1267|Film-NoirThriller|
|    1|   4999|     F| 56|        13|92407|     909|21941234239631...|Action|Crime|Dram...|19871973199819...|1960|        ComedyDrama|      1079|            Comedy|
|    1|   4186|     M| 25|         7|33308|    1103|14113598325420...|DramaDramaComed...|19962000199319...|1955|               Drama|       491|             Drama|
|    1|   1645|     F| 18|         9|62225|    2518|27233261291814...|Action|Adventure|...|19991992198619...|1982|              Comedy|      3253|            Comedy|
|    1|   1101|     M| 35|         1|06417|     446|12501968212243...|Drama|WarComedy|...|19571985198419...|1993|       DramaRomance|      3210|            Comedy|
|    0|   1015|     M| 35|         3|11220|     335|1252161790350...|Film-Noir|Mystery...|19741997195819...|1995|    MysteryThriller|      1249|          Thriller|
|    0|   5843|     M| 35|         1|83301|    3210|12582716921292...|HorrorComedy|Hor...|19801984198219...|1982|              Comedy|      1036|   ActionThriller|
|    0|    550|     M| 45|         8|21559|     778|27451099650202...|DramaDramaDrama...|19861938199619...|1996|               Drama|       458|     DramaWestern|
|    1|   1874|     M| 45|         7|95076|    1034|2601225858162...|Action|Adventure|...|19771984197219...|1996|               Crime|      3469|             Drama|
+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+--------------------+----+--------------------+----------+------------------+
only showing top 10 rows

Debug--match train test dataset sample:
[Stage 4:==================================================================================================================================================================================>     (32 + 1) / 33]                                                                                                                                                                                                               [WARN] 2024-07-08 05:37:43.774 STSAssumeRoleWithWebIdentityCredentialsProvider [140172735100736] Token file must be specified to use STS AssumeRole web identity creds provider.
[2024-07-08 05:37:43.779] [info] [s3_sdk_filesys.cpp:357] Try to open S3 stream: s3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/wide_combine_column_schema, read_only true
[2024-07-08 05:37:43.804] [info] [s3_sdk_filesys.cpp:380] Opened read-only stream for object: s3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/wide_combine_column_schema with total length 17
[2024-07-08 05:37:43.808] [info] [s3_sdk_filesys.cpp:419] Read S3 object s3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/wide_combine_column_schema with size 17 at position 0 larger than total size: 17, change size to 17
[2024-07-08 05:37:43.820] [info] [s3_sdk_filesys.cpp:413] Read S3 object s3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/wide_combine_column_schema reached end 17
[2024-07-08 05:37:43.820] [info] [s3_sdk_filesys.cpp:413] Read S3 object s3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/wide_combine_column_schema reached end 17
user_id
[2024-07-08 05:37:43.821] [info] add expr bkdr_hash(user_id, StringBKDRHashFunctionOption::name=user_id)
movie_id
[2024-07-08 05:37:43.821] [info] add expr bkdr_hash(movie_id, StringBKDRHashFunctionOption::name=movie_id)
[WARN] 2024-07-08 05:37:43.864 STSAssumeRoleWithWebIdentityCredentialsProvider [140172735100736] Token file must be specified to use STS AssumeRole web identity creds provider.
[2024-07-08 05:37:43.868] [info] [s3_sdk_filesys.cpp:357] Try to open S3 stream: s3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/combine_column_schema, read_only true
[2024-07-08 05:37:43.892] [info] [s3_sdk_filesys.cpp:380] Opened read-only stream for object: s3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/combine_column_schema with total length 17
[2024-07-08 05:37:43.895] [info] [s3_sdk_filesys.cpp:419] Read S3 object s3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/combine_column_schema with size 17 at position 0 larger than total size: 17, change size to 17
[2024-07-08 05:37:43.908] [info] [s3_sdk_filesys.cpp:413] Read S3 object s3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/combine_column_schema reached end 17
[2024-07-08 05:37:43.909] [info] [s3_sdk_filesys.cpp:413] Read S3 object s3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/combine_column_schema reached end 17
user_id
[2024-07-08 05:37:43.909] [info] add expr bkdr_hash(user_id, StringBKDRHashFunctionOption::name=user_id)
movie_id
[2024-07-08 05:37:43.909] [info] add expr bkdr_hash(movie_id, StringBKDRHashFunctionOption::name=movie_id)
[2024-07-08 05:37:44.068] [info] PS job with coordinator address 172.16.14.249:52063 started.
[2024-07-08 05:37:44.068] [info] PSRunner::RunPS: pid: 14838, tid: 16023, thread: 0x7f7bc61fc700
[2024-07-08 05:37:44.068] [info] PSRunner::RunPSCoordinator: pid: 14838, tid: 16023, thread: 0x7f7bc61fc700
[2024-07-08 05:37:44.069] [info] ActorProcess::Receiving: Coordinator pid: 14838, tid: 16028, thread: 0x7f7bbf5fe700
[Stage 13:>                                                                                (0 + 2) / 2][Stage 14:>                                                                                (0 + 2) / 2]/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for StringBKDRHashFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for StringBKDRHashFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for BKDRHashCombineFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for BKDRHashCombineFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for StringBKDRHashFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for BKDRHashCombineFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for StringBKDRHashFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for BKDRHashCombineFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
[2024-07-08 05:37:45.336] [info] PS job with coordinator address 172.16.14.249:52063 started.
[2024-07-08 05:37:45.336] [info] PSRunner::RunPS: pid: 16040, tid: 16040, thread: 0x7f6d6f8f7740
[2024-07-08 05:37:45.336] [info] PSRunner::RunPSServer: pid: 16040, tid: 16040, thread: 0x7f6d6f8f7740
[2024-07-08 05:37:45.336] [info] PS job with coordinator address 172.16.14.249:52063 started.
[2024-07-08 05:37:45.336] [info] PSRunner::RunPS: pid: 16035, tid: 16062, thread: 0x7f6d3b068700
[2024-07-08 05:37:45.336] [info] PSRunner::RunPSWorker: pid: 16035, tid: 16062, thread: 0x7f6d3b068700
[38;5;046mps agent registered for process 16035 thread 0x7f6d6f8f7740[m
[2024-07-08 05:37:45.337] [info] ActorProcess::Receiving: Server pid: 16040, tid: 16067, thread: 0x7f6d33fff700
[2024-07-08 05:37:45.337] [info] ActorProcess::Receiving: Worker pid: 16035, tid: 16068, thread: 0x7f6d39265700
[2024-07-08 05:37:45.340] [info] PS job with coordinator address 172.16.14.249:52063 started.
[2024-07-08 05:37:45.340] [info] PSRunner::RunPS: pid: 16045, tid: 16045, thread: 0x7f6d6f8f7740
[2024-07-08 05:37:45.340] [info] PSRunner::RunPSServer: pid: 16045, tid: 16045, thread: 0x7f6d6f8f7740
[2024-07-08 05:37:45.340] [info] ActorProcess::Receiving: Server pid: 16045, tid: 16071, thread: 0x7f6d33fff700
[2024-07-08 05:37:45.341] [info] PS job with coordinator address 172.16.14.249:52063 started.
[2024-07-08 05:37:45.341] [info] PSRunner::RunPS: pid: 16036, tid: 16072, thread: 0x7f6d3b068700
[2024-07-08 05:37:45.341] [info] PSRunner::RunPSWorker: pid: 16036, tid: 16072, thread: 0x7f6d3b068700
[38;5;046mps agent registered for process 16036 thread 0x7f6d6f8f7740[m
[2024-07-08 05:37:45.342] [info] ActorProcess::Receiving: Worker pid: 16036, tid: 16075, thread: 0x7f6d39265700
[2024-07-08 05:37:45.342] [info] C[0]:9: The coordinator has connected to 2 servers and 2 workers.
[2024-07-08 05:37:45.343] [info] W[0]:12 has connected to others.
[2024-07-08 05:37:45.343] [info] W[1]:28 has connected to others.
[2024-07-08 05:37:45.343] [info] S[0]:10 has connected to others.
[2024-07-08 05:37:45.343] [info] S[1]:26 has connected to others.
PS Worker node [38;5;051mW[0]:12[m is ready.
PS Worker node [38;5;051mW[1]:28[m is ready.
PS Server node [38;5;196mS[0]:10[m is ready.
PS Server node [38;5;196mS[1]:26[m is ready.
                                                                                                                                                                                                               user_id
[2024-07-08 05:37:45.404] [info] add expr bkdr_hash(user_id, StringBKDRHashFunctionOption::name=user_id)
movie_id
[2024-07-08 05:37:45.404] [info] add expr bkdr_hash(movie_id, StringBKDRHashFunctionOption::name=movie_id)
user_id
[2024-07-08 05:37:45.404] [info] add expr bkdr_hash(user_id, StringBKDRHashFunctionOption::name=user_id)
movie_id
[2024-07-08 05:37:45.404] [info] add expr bkdr_hash(movie_id, StringBKDRHashFunctionOption::name=movie_id)
user_id
[2024-07-08 05:37:45.405] [info] add expr bkdr_hash(user_id, StringBKDRHashFunctionOption::name=user_id)
movie_id
[2024-07-08 05:37:45.405] [info] add expr bkdr_hash(movie_id, StringBKDRHashFunctionOption::name=movie_id)
user_id
[2024-07-08 05:37:45.405] [info] add expr bkdr_hash(user_id, StringBKDRHashFunctionOption::name=user_id)
movie_id
[2024-07-08 05:37:45.405] [info] add expr bkdr_hash(movie_id, StringBKDRHashFunctionOption::name=movie_id)
[38;5;196minit sparse tensor lr_sparse with slice shape (10,), updater FTRLTensorUpdater(1.0, 1.0, 0.02, 1.0) and initializer NormalTensorInitializer(0.0, 0.01)[m[38;5;196minit sparse tensor lr_sparse with slice shape (10,), updater FTRLTensorUpdater(1.0, 1.0, 0.02, 1.0) and initializer NormalTensorInitializer(0.0, 0.01)[m

[38;5;196minit sparse tensor dnn_sparse with slice shape (10,), updater FTRLTensorUpdater(1.0, 1.0, 0.02, 1.0) and initializer NormalTensorInitializer(0.0, 0.01)[m
[38;5;196minit sparse tensor dnn_sparse with slice shape (10,), updater FTRLTensorUpdater(1.0, 1.0, 0.02, 1.0) and initializer NormalTensorInitializer(0.0, 0.01)[m
[38;5;046minit dense tensor dnn.dnn.0.weight with shape (20, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer OneTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.0.weight with shape (20, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer OneTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.0.bias with shape (20, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer ZeroTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.0.bias with shape (20, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer ZeroTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.1.weight with shape (1024, 20), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.1.weight with shape (1024, 20), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.1.bias with shape (1024, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.1.bias with shape (1024, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.3.weight with shape (512, 1024), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.3.weight with shape (512, 1024), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.3.bias with shape (512, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.3.bias with shape (512, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.5.weight with shape (256, 512), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.5.weight with shape (256, 512), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.5.bias with shape (256, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.5.bias with shape (256, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.7.weight with shape (128, 256), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.7.weight with shape (128, 256), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.7.bias with shape (128, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.7.bias with shape (128, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.9.weight with shape (1, 128), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.9.weight with shape (1, 128), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.9.bias with shape (1, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;046minit dense tensor dnn.dnn.9.bias with shape (1, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()[m
[38;5;051minit dense tensor dnn.dnn.0.running_mean with shape (20, 1), updater EMATensorUpdater(0.1) and initializer ZeroTensorInitializer()[m
[38;5;051minit dense tensor dnn.dnn.0.running_mean with shape (20, 1), updater EMATensorUpdater(0.1) and initializer ZeroTensorInitializer()[m
[38;5;051minit dense tensor dnn.dnn.0.running_var with shape (20, 1), updater EMATensorUpdater(0.1) and initializer OneTensorInitializer()[m
[38;5;051minit dense tensor dnn.dnn.0.running_var with shape (20, 1), updater EMATensorUpdater(0.1) and initializer OneTensorInitializer()[m
[Stage 14:>                                                                                                                                                                                        (0 + 2) / 2][Stage 14:>                                                                                (0 + 2) / 2][Stage 29:>                                                                                (0 + 1) / 1]/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for StringBKDRHashFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for BKDRHashCombineFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
24/07/08 05:37:48 ERROR Utils: Aborting task
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 382, in _feed_training_minibatch
    self = __class__.get_instance()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 160, in get_instance
    return instance
RuntimeError: no ps agent registered for thread 0x7f134cec6740 on pid 16365

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/07/08 05:37:48 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 135, attempt 0, stage 29.0)
24/07/08 05:37:48 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 135, attempt 0, stage 29.0)
24/07/08 05:37:48 ERROR Executor: Exception in task 0.0 in stage 29.0 (TID 135)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 382, in _feed_training_minibatch
    self = __class__.get_instance()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 160, in get_instance
    return instance
RuntimeError: no ps agent registered for thread 0x7f134cec6740 on pid 16365

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/07/08 05:37:48 WARN TaskSetManager: Lost task 0.0 in stage 29.0 (TID 135) (ip-172-16-14-249.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 382, in _feed_training_minibatch
    self = __class__.get_instance()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 160, in get_instance
    return instance
RuntimeError: no ps agent registered for thread 0x7f134cec6740 on pid 16365

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

24/07/08 05:37:48 ERROR TaskSetManager: Task 0 in stage 29.0 failed 1 times; aborting job
24/07/08 05:37:48 ERROR OverwriteByExpressionExec: Data source write support org.apache.spark.sql.execution.datasources.noop.NoopBatchWrite$@44275ec2 is aborting.
24/07/08 05:37:48 ERROR OverwriteByExpressionExec: Data source write support org.apache.spark.sql.execution.datasources.noop.NoopBatchWrite$@44275ec2 aborted.
[2024-07-08 05:37:48.359] [error] RunPSCoordinator: Py4JJavaError: An error occurred while calling o292.save.
: org.apache.spark.SparkException: Writing job aborted
	at org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:767)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:409)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:353)
	at org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:262)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:332)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:331)
	at org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:262)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:318)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 135) (ip-172-16-14-249.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 382, in _feed_training_minibatch
    self = __class__.get_instance()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 160, in get_instance
    return instance
RuntimeError: no ps agent registered for thread 0x7f134cec6740 on pid 16365

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2604)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2603)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2603)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1178)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1178)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1178)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2798)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2787)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:377)
	... 44 more
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 382, in _feed_training_minibatch
    self = __class__.get_instance()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 160, in get_instance
    return instance
RuntimeError: no ps agent registered for thread 0x7f134cec6740 on pid 16365

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more


At:
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/py4j/protocol.py(326): get_return_value
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyspark/sql/utils.py(198): deco
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/py4j/java_gateway.py(1321): __call__
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyspark/sql/readwriter.py(966): save
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/estimator.py(375): _default_feed_training_dataset
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/estimator.py(365): feed_training_dataset
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/estimator.py(356): feed_dataset
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/estimator.py(97): run
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py(274): launch_coordinator
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/threading.py(870): run
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/threading.py(932): _bootstrap_inner
  /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/threading.py(890): _bootstrap

[2024-07-08 05:37:48.360] [info] W[0]:12 has stopped.
[2024-07-08 05:37:48.360] [info] W[1]:28 has stopped.
[2024-07-08 05:37:48.360] [info] S[0]:10 has stopped.
[2024-07-08 05:37:48.360] [info] S[1]:26 has stopped.
/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for StringBKDRHashFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for BKDRHashCombineFunctionOption not exposed
  warnings.warn("Python binding for {} not exposed"
+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+--------------------+----+--------------------+----------+--------------------+
|label|user_id|gender|age|occupation|  zip|movie_id|    recent_movie_ids| recent_movie_genres|  recent_movie_years|year|               genre|last_movie|          last_genre|
+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+--------------------+----+--------------------+----------+--------------------+
|    1|   1315|     M| 35|         7|75214|    1269|91929612133783...|Adventure|Childre...|19391994199019...|1944|ComedyMysteryTh...|      3317|        ComedyDrama|
|    1|    425|     M| 25|        12|55303|    1394|14524704712112...|ActionAdventure|...|19951986199419...|1987|              Comedy|      1517|              Comedy|
|    1|   4507|     M| 18|         4|61820|    2492|15134481298780...|Drama|Romance|War...|19951987198219...|1998|              Comedy|      2012|ComedySci-FiWes...|
|    0|   2302|     M| 50|        12|48104|    2212|12339241299296...|Action|Drama|War...|19811968198419...|1934|            Thriller|      3363|        ComedyDrama|
|    1|   2295|     M| 45|        11|72316|    1200|13673534284624...|Children's|Comedy...|19962000198619...|1986|ActionSci-FiThr...|      1214|ActionHorrorSci...|
|    0|   4172|     M| 35|         7|19901|    2602|17291721122185...|Crime|DramaDrama...|19971997197419...|1977|    AdventureSci-Fi|      3174|        ComedyDrama|
|    1|   5991|     F| 35|        20|94025|    2273|34381204300017...|Action|Children's...|19901962199719...|1998|     ActionThriller|      1339|      HorrorRomance|
|    1|    892|     M| 35|         0|76031|     924|35620801079130...|Comedy|Romance|Wa...|19941955198819...|1968|DramaMysterySci...|      1210|ActionAdventure...|
|    1|   4966|     M| 50|        14|55407|    3755|29512120003635...|Action|WesternAc...|19641995198719...|2000|ActionAdventure...|      1376|ActionAdventure...|
|    0|   1099|     M| 25|        17|77059|     502|11001917208485...|Action|RomanceAc...|19901998199519...|1994|   ActionChildren's|      3705|ActionAdventure...|
+-----+-------+------+---+----------+-----+--------+--------------------+--------------------+--------------------+----+--------------------+----------+--------------------+
only showing top 10 rows

Debug -- train dataset positive count: 82
Debug -- train dataset negative count: 18
Debug -- test dataset count: 100
[32mloaded combine schema from[m [32mcombine schema file [m's3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/wide_combine_column_schema'
[32mloaded combine schema from[m [32mcombine schema file [m's3://sagemaker-us-west-2-452145973879/datasets/movielens/schema/ml_1m/combine_column_schema'
PS Coordinator node [32mC[0]:9[m is ready.
[2024-07-08 05:37:48.360] [info] PS job with coordinator address 172.16.14.249:52063 stopped.
[2024-07-08 05:37:48.360] [info] PS job with coordinator address 172.16.14.249:52063 stopped.
[38;5;196mps agent deregistered for process 16035 thread 0x7f6d6f8f7740[m[38;5;196mps agent deregistered for process 16036 thread 0x7f6d6f8f7740[m

[2024-07-08 05:37:48.360] [info] PS job with coordinator address 172.16.14.249:52063 stopped.
[2024-07-08 05:37:48.360] [info] PS job with coordinator address 172.16.14.249:52063 stopped.
Traceback (most recent call last):
  File "widedeep.py", line 137, in <module>
    model = train(spark, train_dataset, **params)
  File "widedeep.py", line 103, in train
    model = estimator.fit(train_dataset)
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyspark/ml/base.py", line 205, in fit
    return self._fit(dataset)
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/estimator.py", line 965, in _fit
    launcher.launch()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/estimator.py", line 608, in launch
    self.launch_agent()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/ps_launcher.py", line 144, in launch_agent
    asyncio.run(class_._launch(args, spark_session, self))
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/nest_asyncio.py", line 31, in run
    return loop.run_until_complete(task)
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/nest_asyncio.py", line 99, in run_until_complete
    return f.result()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/asyncio/futures.py", line 178, in result
    raise self._exception
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/asyncio/tasks.py", line 282, in __step
    result = coro.throw(exc)
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 326, in _launch
    await asyncio.gather(*futures)
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/asyncio/tasks.py", line 349, in __wakeup
    future.result()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 274, in launch_coordinator
    PSRunner.run_ps(conf)
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/estimator.py", line 97, in run
    self.feed_dataset()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/estimator.py", line 356, in feed_dataset
    self.feed_training_dataset()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/estimator.py", line 365, in feed_training_dataset
    self._default_feed_training_dataset()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/estimator.py", line 375, in _default_feed_training_dataset
    df.write.format('noop').mode('overwrite').save()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyspark/sql/readwriter.py", line 966, in save
    self._jwrite.save()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/py4j/java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyspark/sql/utils.py", line 190, in deco
    return f(*a, **kw)
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o292.save.
: org.apache.spark.SparkException: Writing job aborted
	at org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:767)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:409)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:353)
	at org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:262)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:332)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:331)
	at org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:262)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:318)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 135) (ip-172-16-14-249.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 382, in _feed_training_minibatch
    self = __class__.get_instance()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 160, in get_instance
    return instance
RuntimeError: no ps agent registered for thread 0x7f134cec6740 on pid 16365

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2604)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2603)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2603)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1178)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1178)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1178)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2798)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2787)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:377)
	... 44 more
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 382, in _feed_training_minibatch
    self = __class__.get_instance()
  File "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py", line 160, in get_instance
    return instance
RuntimeError: no ps agent registered for thread 0x7f134cec6740 on pid 16365

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:136)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more

[Stage 14:>                                                                                                                                                                                        (0 + 2) / 2]24/07/08 05:37:48 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 0
java.net.SocketException: Connection reset
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:200)
	at org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:734)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1606)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1597)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1597)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1745)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2853)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2798)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2787)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
24/07/08 05:37:48 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 1
java.net.SocketException: Broken pipe (Write failed)
	at java.base/java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
	at java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at java.base/java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:732)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1606)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1597)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1597)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1745)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2853)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2798)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2787)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
                                                                                                                                                                                                               