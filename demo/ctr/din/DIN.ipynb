{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7b8672-5ae6-4d05-aa05-edc38e491463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import metaspore as ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8cc895-c6ce-46e6-8420-2cc1e69d0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_ROOT_DIR = 's3://dmetasoul-bucket/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d822fe-3150-4175-baae-84227422f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dice(nn.Module):\n",
    "    \"\"\"\n",
    "    The Data Adaptive Activation Function in DIN, a generalization of PReLu.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size, dim=2, epsilon=1e-8):\n",
    "        super(Dice, self).__init__()\n",
    "        assert dim == 2 or dim == 3\n",
    " \n",
    "        self.bn = nn.BatchNorm1d(emb_size, eps=epsilon)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # wrap alpha in nn.Parameter to make it trainable\n",
    "        self.alpha = nn.Parameter(torch.zeros((emb_size,))) if self.dim == 2 else nn.Parameter(\n",
    "            torch.zeros((emb_size, 1)))\n",
    " \n",
    " \n",
    "    def forward(self, x):\n",
    "        assert x.dim() == self.dim\n",
    "        if self.dim == 2:\n",
    "            x_p = self.sigmoid(self.bn(x))\n",
    "            out = self.alpha * (1 - x_p) * x + x_p * x\n",
    "        else:\n",
    "            x = torch.transpose(x, 1, 2)\n",
    "            x_p = self.sigmoid(self.bn(x))\n",
    "            out = self.alpha * (1 - x_p) * x + x_p * x\n",
    "            out = torch.transpose(out, 1, 2)\n",
    "        return out\n",
    "\n",
    "def init_weights(model):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        if model.weight is not None:\n",
    "            nn.init.kaiming_uniform_(model.weight.data)\n",
    "        if model.bias is not None:\n",
    "            nn.init.normal_(model.bias.data)\n",
    "    elif isinstance(model, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d)):\n",
    "        if model.weight is not None:\n",
    "            nn.init.normal_(model.weight.data, mean=1, std=0.02)\n",
    "        if model.bias is not None:\n",
    "            nn.init.constant_(model.bias.data, 0)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1395fc-46d1-4875-a7a6-125a3abcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers,\n",
    "                 dropout=0.0, batchnorm=True, activation='dice'):\n",
    "        super(MLP, self).__init__()\n",
    "        from collections import OrderedDict\n",
    "        modules = OrderedDict()\n",
    "        previous_size = input_size\n",
    "        for index, hidden_layer in enumerate(hidden_layers):\n",
    "            modules[f\"dense{index}\"] = nn.Linear(previous_size, hidden_layer)\n",
    "            if batchnorm:\n",
    "                modules[f\"batchnorm{index}\"] = nn.BatchNorm1d(hidden_layer)\n",
    "            if activation:\n",
    "                modules[f\"activation{index}\"] = Dice(hidden_layer) # 直接使用dice，不用get_activation_layer函数\n",
    "            if dropout:\n",
    "                modules[f\"dropout{index}\"] = nn.Dropout(dropout)\n",
    "            previous_size = hidden_layer\n",
    "        self.mlp = nn.Sequential(modules)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "    \n",
    "# def get_activation_layer(name, hidden_size=None, dice_dim=2):\n",
    "#     name = name.lower()\n",
    "#     name_dict = {x.lower():x for x in dir(nn) if '__' not in x and 'Z'>=x[0]>='A'}\n",
    "#     if name==\"linear\":\n",
    "#         return Identity()\n",
    "#     elif name==\"dice\":\n",
    "#         assert dice_dim\n",
    "#         return Dice(hidden_size, dice_dim)\n",
    "#     else:\n",
    "#         assert name in name_dict, f'activation type {name} not supported!'\n",
    "#         return getattr(nn,name_dict[name])()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326575e4-4a8d-4426-8fe2-17be0d808e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_layers,\n",
    "            dropout=0.0,\n",
    "            batchnorm=True,\n",
    "            return_scores=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.return_scores = return_scores\n",
    "        \n",
    "        self.mlp = MLP(\n",
    "            input_size=input_size * 4,\n",
    "            hidden_layers=hidden_layers,\n",
    "            dropout=dropout,\n",
    "            batchnorm=batchnorm,\n",
    "            activation='dice')\n",
    "        self.fc = nn.Linear(hidden_layers[-1], 1)\n",
    " \n",
    "    def forward(self, query, keys, keys_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: 2D tensor, [Batch, Hidden]\n",
    "        keys: 3D tensor, [Batch, Time, Hidden]\n",
    "        keys_length: 1D tensor, [Batch]\n",
    "        Returns\n",
    "        -------\n",
    "        outputs: 2D tensor, [Batch, Hidden]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, max_length, dim = keys.size()\n",
    "        \n",
    "        query = query.unsqueeze(1).expand(-1, max_length, -1)\n",
    " \n",
    "        din_all = torch.cat(\n",
    "            [query, keys, query - keys, query * keys], dim=-1)\n",
    " \n",
    "        din_all = din_all.view(batch_size * max_length, -1) # [B*T 4*H]\n",
    "        \n",
    "        outputs = self.mlp(din_all)\n",
    " \n",
    "        outputs = self.fc(outputs).view(batch_size, max_length)  # [B, T]\n",
    " \n",
    "        # Scale\n",
    "        outputs = outputs / (dim ** 0.5)\n",
    " \n",
    "        # Mask\n",
    "        mask = (torch.arange(max_length, device=keys_length.device).repeat(\n",
    "            batch_size, 1) < keys_length.view(-1, 1))\n",
    "        outputs[~mask] = -np.inf\n",
    " \n",
    "        # Activation\n",
    "        outputs = torch.sigmoid(outputs)  # [B, T]\n",
    " \n",
    "        if not self.return_scores:\n",
    "            # Weighted sum\n",
    "            outputs = torch.matmul(\n",
    "                outputs.unsqueeze(1), keys).squeeze()  # [B, H]\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3ac857-d448-4ea5-af0e-d05c0fc4f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "3 4\n"
     ]
    }
   ],
   "source": [
    "# model = Attention(16,\n",
    "#             [16, 8],\n",
    "#             dropout=0.0,\n",
    "#             batchnorm=True,\n",
    "#             return_scores=False)\n",
    "# query = torch.ones(100, 2, 16)\n",
    "# keys = torch.ones(100, 10, 16)\n",
    "# keys_length = torch.ones(100)*10\n",
    "# print(query.shape)\n",
    "# print(keys.shape)\n",
    "# print(keys_length.shape)\n",
    "# model(query, keys, keys_length)\n",
    "seq_column_index_list = [1, 3]\n",
    "target_column_index_list = [2, 4]\n",
    "for seq_column_index, target_column_index in zip(seq_column_index_list, target_column_index_list):\n",
    "    print(seq_column_index, target_column_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9e7d1f2-3c53-4714-af83-3435de1c13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import metaspore as ma\n",
    "import copy\n",
    "class DIN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._embedding_size = 16\n",
    "        self._schema_dir = S3_ROOT_DIR + 'demo/movielens/schema/'\n",
    "        \n",
    "        self._column_name_path =  'column_schema_group_attention.txt'\n",
    "        self._combine_schema_path =  'combine_column_schema_group_attention.txt'\n",
    "        self._embedding_table = ma.EmbeddingLookup(self._embedding_size, self._column_name_path, self._combine_schema_path)\n",
    "        self._embedding_table.updater = ma.FTRLTensorUpdater()\n",
    "        self._embedding_table.initializer = ma.NormalTensorInitializer(var=0.01)\n",
    "        \n",
    "        self.attention_hidden_layers=[16, 8]\n",
    "        self.batchnorm=True\n",
    "        self.return_score=False\n",
    "        self._attention = Attention(self._embedding_size,\n",
    "                                    hidden_layers=[16,8],\n",
    "                                    dropout=0.1,\n",
    "                                    batchnorm=True,\n",
    "                                    return_scores=False)\n",
    "        # user_embedding + sum_pooling + target_embedding\n",
    "        self.feature_num = 5\n",
    "        total_input_size = self._embedding_size * self.feature_num\n",
    "        mlp_hidden_layers=[32, 16]\n",
    "        self.mlp = MLP(input_size=total_input_size,\n",
    "            hidden_layers=mlp_hidden_layers,\n",
    "            dropout=0.25, batchnorm=True, activation=\"dice\")\n",
    "        self.final_layer = nn.Linear(mlp_hidden_layers[-1], 1)\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "        self._sparse = ma.EmbeddingSumConcat(self._embedding_size, self._column_name_path, self._combine_schema_path)\n",
    "        self._sparse.updater = ma.FTRLTensorUpdater()\n",
    "        self._sparse.initializer = ma.NormalTensorInitializer(var=0.01)\n",
    "        self._dense = torch.nn.Sequential(\n",
    "            ma.nn.Normalization(self._sparse.feature_count * self._embedding_size),\n",
    "            torch.nn.Linear(self._sparse.feature_count * self._embedding_size, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, offset = self._embedding_table(x) \n",
    "        # print(\"offset: \", offset)\n",
    "        \n",
    "        all_column = self.feature_num\n",
    "        # 利用offset计算取出每个feature的embedding\n",
    "        x_reshape = [x[offset[i]:offset[i+1],:] for i in range(offset.shape[0]-1)]\n",
    "        x_reshape.append(x[offset[offset.shape[0]-1]:x.shape[0],:])\n",
    "        # print(\"x_reshape: \", len(x_reshape)) # batch_size * feature数量\n",
    "        user_column_index = 0\n",
    "        user_embedding = x_reshape[0::all_column]\n",
    "        # print(\"user_embdding:\", len(user_embedding)) # 100\n",
    "        user_embedding = torch.nn.utils.rnn.pad_sequence(user_embedding, batch_first = True) # [B 1 H]\n",
    "        user_embedding = user_embedding.squeeze() \n",
    "        # print(\"user_embedding.shape: \", user_embedding.shape) # [B H]\n",
    "            \n",
    "        # attention group\n",
    "        seq_column_index_list = [1, 3]\n",
    "        target_column_index_list = [2, 4]\n",
    "        all_sum_pooling=None\n",
    "        for seq_column_index, target_column_index in zip(seq_column_index_list, target_column_index_list):\n",
    "            # 计算item_seq_length\n",
    "            item_seq_length = [offset[i] - offset[i-1] for i in range(seq_column_index+1, offset.shape[0], all_column)]\n",
    "            item_seq_length = torch.tensor(item_seq_length) # 维度为[B]\n",
    "            # print(\"item_seq_length: \", item_seq_length)\n",
    "        \n",
    "            # split feature\n",
    "            item_seq_embedding = x_reshape[seq_column_index::all_column] # 长度为100的列表，列表中每个元素就是每个样本的item_seq对应的tensor \n",
    "            # print(item_seq_embedding)\n",
    "            item_seq_embedding = torch.nn.utils.rnn.pad_sequence(item_seq_embedding, batch_first = True)\n",
    "            # print(\"item_seq_embdding.shape: \", item_seq_embedding.shape) # [B T H]\n",
    "\n",
    "            target_item_embedding = x_reshape[target_column_index::all_column]\n",
    "            # print(\"target_item_embedding:\", len(target_item_embedding)) # B\n",
    "            target_item_embedding = torch.nn.utils.rnn.pad_sequence(target_item_embedding, batch_first = True)\n",
    "            target_item_embedding = target_item_embedding.squeeze() \n",
    "            # print(\"target_item_embedding.shape: \", target_item_embedding.shape) # [B H]\n",
    "            sum_pooling = self._attention(target_item_embedding, item_seq_embedding, item_seq_length) \n",
    "            # print(sum_pooling.shape)\n",
    "            if all_sum_pooling == None:\n",
    "                all_sum_pooling = torch.cat((sum_pooling, target_item_embedding), dim=1)\n",
    "            else:\n",
    "                all_sum_pooling = torch.cat((all_sum_pooling, sum_pooling, target_item_embedding), dim=1)\n",
    "            \n",
    "        # print(\"all_sum_pooling.shape: \", all_sum_pooling.shape) # [B H]\n",
    "        emb_concat = torch.cat((user_embedding, all_sum_pooling), dim=1) \n",
    "        # print(\"emb_concat.shape: \", emb_concat.shape) # [B 3*H]\n",
    "        final_layer_inputs = self.mlp(emb_concat) # [B  mlp_hidden_layers[-1]]\n",
    "        output = self.final_layer(final_layer_inputs) # [B 1]\n",
    "        # print(\"output.shape: \", output.shape)\n",
    "        return torch.sigmoid(output) \n",
    "    \n",
    "    \n",
    "        # x = self._sparse(x)\n",
    "        # x = self._dense(x)\n",
    "        # return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e08e2c74-1e33-4b22-93b9-c233ab5cb1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mloaded combine schema from\u001b[m \u001b[32mcolumn name file \u001b[m'column_schema_group_attention.txt' \u001b[32mand combine schema file \u001b[m'combine_column_schema_group_attention.txt'\n",
      "\u001b[32mloaded combine schema from\u001b[m \u001b[32mcolumn name file \u001b[m'column_schema_group_attention.txt' \u001b[32mand combine schema file \u001b[m'combine_column_schema_group_attention.txt'\n"
     ]
    }
   ],
   "source": [
    "module= DIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3869306-b862-445d-8f8c-8f29ad7ea3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(local=True,\n",
    "          batch_size=1000,\n",
    "          worker_count=1,\n",
    "          server_count=1,\n",
    "          worker_cpu=1,\n",
    "          server_cpu=1,\n",
    "          worker_memory='5G',\n",
    "          server_memory='5G',\n",
    "          coordinator_memory='5G',\n",
    "          module_class=None,\n",
    "          model_in_path=None,\n",
    "          model_out_path=None,\n",
    "          model_export_path=None,\n",
    "          model_version=None,\n",
    "          experiment_name=None,\n",
    "          input_label_column_index=0,\n",
    "          delimiter='\\001',\n",
    "          train_dataset_path=None,\n",
    "          test_dataset_path=None,\n",
    "          is_catchup=True,\n",
    "          consul_host=None,\n",
    "          consul_port=None,\n",
    "          consul_endpoint_prefix=None,\n",
    "          max_sparse_feature_age=15,\n",
    "          metric_update_interval=10,\n",
    "         ):\n",
    "    import pyspark\n",
    "    import metaspore as ma\n",
    "    if module_class is None:\n",
    "        module_class = DIN\n",
    "    print('local: %s' % local)\n",
    "    print('batch_size: %d' % batch_size)\n",
    "    print('worker_count: %d' % worker_count)\n",
    "    print('server_count: %d' % server_count)\n",
    "    print('worker_cpu: %d' % worker_cpu)\n",
    "    print('server_cpu: %d' % server_cpu)\n",
    "    print('worker_memory: %s' % worker_memory)\n",
    "    print('server_memory: %s' % server_memory)\n",
    "    print('coordinator_memory: %s' % coordinator_memory)\n",
    "    print('module_class: %s' % module_class)\n",
    "    print('model_in_path: %s' % model_in_path)\n",
    "    print('model_out_path: %s' % model_out_path)\n",
    "    print('model_export_path: %s' % model_export_path)\n",
    "    print('model_version: %s' % model_version)\n",
    "    print('experiment_name: %s' % experiment_name)\n",
    "    print('input_label_column_index: %d' % input_label_column_index)\n",
    "    print('delimiter: %r' % delimiter)\n",
    "    print('train_dataset_path: %s' % train_dataset_path)\n",
    "    print('test_dataset_path: %s' % test_dataset_path)\n",
    "    print('is_catchup: %s' % is_catchup)\n",
    "    print('consul_host: %s' % consul_host)\n",
    "    print('consul_port: %s' % consul_port)\n",
    "    print('consul_endpoint_prefix: %s' % consul_endpoint_prefix)\n",
    "    print('max_sparse_feature_age: %d' % max_sparse_feature_age)\n",
    "    print('metric_update_interval: %d' % metric_update_interval)\n",
    "    module = module_class()\n",
    "    config={}\n",
    "    estimator = ma.PyTorchEstimator(module=module,\n",
    "                                    worker_count=worker_count,\n",
    "                                    server_count=server_count,\n",
    "                                    model_in_path=model_in_path,\n",
    "                                    model_out_path=model_out_path,\n",
    "                                    model_export_path=model_export_path,\n",
    "                                    model_version=model_version,\n",
    "                                    experiment_name=experiment_name,\n",
    "                                    input_label_column_index=input_label_column_index,\n",
    "                                    consul_host=consul_host,\n",
    "                                    consul_port=consul_port,\n",
    "                                    consul_endpoint_prefix=consul_endpoint_prefix,\n",
    "                                    max_sparse_feature_age=max_sparse_feature_age,\n",
    "                                    metric_update_interval=metric_update_interval,\n",
    "                                    **config,\n",
    "                                   )\n",
    "    spark_session = ma.spark.get_session(local=local,\n",
    "                                         batch_size=batch_size,\n",
    "                                         worker_count=estimator.worker_count,\n",
    "                                         server_count=estimator.server_count,\n",
    "                                         worker_cpu=worker_cpu,\n",
    "                                         server_cpu=server_cpu,\n",
    "                                         worker_memory=worker_memory,\n",
    "                                         server_memory=server_memory,\n",
    "                                         coordinator_memory=coordinator_memory,\n",
    "                                        )\n",
    "    with spark_session:\n",
    "        train_dataset = spark_session.read.parquet(train_dataset_path)\n",
    "        model = estimator.fit(train_dataset)\n",
    "        if test_dataset_path is not None:\n",
    "            test_dataset = spark_session.read.parquet(test_dataset_path)\n",
    "            result = model.transform(test_dataset)\n",
    "            evaluator = pyspark.ml.evaluation.BinaryClassificationEvaluator()\n",
    "            test_auc = evaluator.evaluate(result)\n",
    "            print('test_auc: %g' % test_auc)\n",
    "        if not is_catchup and model.consul_endpoint_prefix is not None:\n",
    "            model.publish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52df56-dcea-4a3f-bf95-f80376eec221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local: True\n",
      "batch_size: 1000\n",
      "worker_count: 1\n",
      "server_count: 1\n",
      "worker_cpu: 1\n",
      "server_cpu: 1\n",
      "worker_memory: 5G\n",
      "server_memory: 5G\n",
      "coordinator_memory: 5G\n",
      "module_class: <class '__main__.DIN'>\n",
      "model_in_path: None\n",
      "model_out_path: s3://dmetasoul-bucket/demo/output/dev/model_out/\n",
      "model_export_path: None\n",
      "model_version: None\n",
      "experiment_name: None\n",
      "input_label_column_index: 0\n",
      "delimiter: '\\x01'\n",
      "train_dataset_path: s3://dmetasoul-bucket/demo/movielens/1m/rank/train_DIN_group_attention.parquet\n",
      "test_dataset_path: s3://dmetasoul-bucket/demo/movielens/1m/rank/test_DIN_group_attention.parquet\n",
      "is_catchup: True\n",
      "consul_host: None\n",
      "consul_port: None\n",
      "consul_endpoint_prefix: None\n",
      "max_sparse_feature_age: 15\n",
      "metric_update_interval: 10\n",
      "\u001b[32mloaded combine schema from\u001b[m \u001b[32mcolumn name file \u001b[m'column_schema_group_attention.txt' \u001b[32mand combine schema file \u001b[m'combine_column_schema_group_attention.txt'\n",
      "\u001b[32mloaded combine schema from\u001b[m \u001b[32mcolumn name file \u001b[m'column_schema_group_attention.txt' \u001b[32mand combine schema file \u001b[m'combine_column_schema_group_attention.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/08/24 08:02:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/24 08:02:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_out_path = S3_ROOT_DIR + 'demo/output/dev/model_out/'\n",
    "train_dataset_path = S3_ROOT_DIR + 'demo/movielens/1m/rank/train_DIN_group_attention.parquet'\n",
    "test_dataset_path = S3_ROOT_DIR + 'demo/movielens/1m/rank/test_DIN_group_attention.parquet'\n",
    "# test_dataset_path = None\n",
    "train(model_out_path=model_out_path,\n",
    "      train_dataset_path=train_dataset_path,\n",
    "      test_dataset_path=test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83cd215-1af8-4dce-8b04-54facd0874f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
