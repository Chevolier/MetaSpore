{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e8ba8f",
   "metadata": {},
   "source": [
    "# Tutorial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfef096-b2ed-423d-9956-2cb6f8f18988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# data_path = \"data/train/day_0_0.001_train.csv\"\n",
    "\n",
    "# df = pd.read_csv(data_path, header=None)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a3c62-cbd0-4cfe-9888-e84ae4c8ae7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example = df.loc[1, 0]\n",
    "# example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b3fef-cca8-4543-9926-c70bc5bfbfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features = example.split('\\t')\n",
    "# len(features)\n",
    "# features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045523d-1d6e-45a5-8578-7c74cbd561e4",
   "metadata": {},
   "source": [
    "# Read s3 orc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b11f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = []\n",
    "# with open(f'./schema/column_name_mobivista.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         column_names.append(line.split(' ')[1].strip())\n",
    "# print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b872c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metaspore as ms\n",
    "\n",
    "# # train_dataset_path = ROOT_DIR + '/data/train/day_0_0.001_train.csv'\n",
    "\n",
    "# file_base_path = 's3://mv-mtg-di-for-poc-datalab/2024/06/14/00/'\n",
    "# num_files = 10\n",
    "# file_names = [f'part-{str(i).zfill(5)}-1e73cc51-9b17-4439-9d71-7d505df2cae3-c000.snappy.orc' for i in range(num_files)]\n",
    "# train_dataset_path = [file_base_path + file_name for file_name in file_names]\n",
    "\n",
    "# # train_dataset_path = 's3://mv-mtg-di-for-poc-datalab/2024/06/14/00/part-00000-1e73cc51-9b17-4439-9d71-7d505df2cae3-c000.snappy.orc'\n",
    "# # train_dataset_path = 's3://mv-mtg-di-for-poc-datalab/2024/06/14/00/'\n",
    "\n",
    "# spark_confs = {\n",
    "#     'spark.eventLog.enabled':'true',\n",
    "#     'spark.executor.memory': '10g',\n",
    "#     'spark.driver.memory': '10g',\n",
    "#     \"spark.driver.maxResultSize\": \"10g\",\n",
    "#     \"spark.sql.files.ignoreCorruptFiles\": \"true\"\n",
    "# }\n",
    "\n",
    "# spark_session = ms.spark.get_session(local=True,\n",
    "#                                      batch_size=100,\n",
    "#                                      worker_count=2,\n",
    "#                                      server_count=2,\n",
    "#                                      log_level='WARN',\n",
    "#                                      spark_confs=spark_confs)\n",
    "\n",
    "# train_dataset = ms.input.read_s3_csv(spark_session, \n",
    "#                                      train_dataset_path, \n",
    "#                                      format='orc',\n",
    "#                                      shuffle=False,\n",
    "#                                      delimiter='\\t', \n",
    "#                                      multivalue_delimiter=\"\\001\", \n",
    "#                                      column_names=column_names,\n",
    "#                                      multivalue_column_names=column_names[:-1])\n",
    "\n",
    "# # train_dataset = spark_session.read.orc(train_dataset_path)\n",
    "\n",
    "# # train_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# df = train_dataset.toPandas()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_positive = df[df['label']=='1'].shape[0]\n",
    "# num_negative = df[df['label']=='0'].shape[0]\n",
    "# print(f\"num_positive: {num_positive}, num_negative: {num_negative}, ratio: {num_positive/num_negative*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = df.columns.tolist()\n",
    "# print(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_hash = 0\n",
    "# feature_hash = {}\n",
    "# feature_value = {}\n",
    "\n",
    "# for i, row in df.iterrows():\n",
    "#     for column in columns[:-1]:\n",
    "#         if column in feature_value:\n",
    "#             feature_value[column].append(row[column].shape[0])\n",
    "#         else:\n",
    "#             feature_value[column] = [row[column].shape[0]]\n",
    "        \n",
    "#         for item in row[column]:\n",
    "#             if '\\003' in item:\n",
    "#                 hash, weight = item.split('\\003')\n",
    "#             else:\n",
    "#                 hash, weight = '', 0\n",
    "#             if column in feature_hash:\n",
    "#                 feature_hash[column].add(hash)\n",
    "#             else:\n",
    "#                 feature_hash[column] = set(hash)\n",
    "\n",
    "# df_feature = pd.DataFrame(feature_value)\n",
    "# df_feature.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_hash_count = dict([(k, [len(v)]) for k, v in feature_hash.items()])\n",
    "# df_hash = pd.DataFrame(feature_hash_count)\n",
    "# df_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_hash = df_hash.sum(axis=1).values[0]\n",
    "# print(f\"Number of total hashes: {total_hash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08cbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "with open(f'./schema/column_name_mobivista.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        columns.append(line.split(' ')[1].strip())\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaspore as ms\n",
    "\n",
    "# Assuming column_names and columns variables are already defined\n",
    "file_base_path = 's3://mv-mtg-di-for-poc-datalab/2024/06/14/00/'\n",
    "num_files = 10\n",
    "file_names = [f'part-{str(i).zfill(5)}-1e73cc51-9b17-4439-9d71-7d505df2cae3-c000.snappy.orc' for i in range(num_files)]\n",
    "train_dataset_path = [file_base_path + file_name for file_name in file_names]\n",
    "\n",
    "spark_confs = {\n",
    "    'spark.eventLog.enabled': 'true',\n",
    "    'spark.executor.memory': '10g',\n",
    "    'spark.driver.memory': '10g',\n",
    "    \"spark.driver.maxResultSize\": \"10g\",\n",
    "    \"spark.sql.files.ignoreCorruptFiles\": \"true\"\n",
    "}\n",
    "\n",
    "spark_session = ms.spark.get_session(local=True,\n",
    "                                     batch_size=100,\n",
    "                                     worker_count=2,\n",
    "                                     server_count=2,\n",
    "                                     log_level='WARN',\n",
    "                                     spark_confs=spark_confs)\n",
    "\n",
    "train_dataset = ms.input.read_s3_csv(spark_session, \n",
    "                                     train_dataset_path, \n",
    "                                     format='orc',\n",
    "                                     shuffle=False,\n",
    "                                     delimiter='\\t', \n",
    "                                     multivalue_delimiter=\"\\001\", \n",
    "                                     column_names=column_names,\n",
    "                                     multivalue_column_names=column_names[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import udf, col, sum as spark_sum\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n",
    "\n",
    "# UDF to calculate feature values and hash count\n",
    "def process_row(column_values):\n",
    "    feature_values = []\n",
    "    feature_hashes = set()\n",
    "    for value in column_values:\n",
    "        if '\\003' in value:\n",
    "            hash_val, weight = value.split('\\003')\n",
    "        else:\n",
    "            hash_val, weight = '', 0\n",
    "        feature_values.append(len(value))\n",
    "        feature_hashes.add(hash_val)\n",
    "    return (feature_values, len(feature_hashes))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"feature_values\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"hash_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "process_row_udf = udf(process_row, schema)\n",
    "\n",
    "# Applying UDF on each column\n",
    "for column in columns[:-1]:\n",
    "    train_dataset = train_dataset.withColumn(f'{column}_processed', process_row_udf(col(column)))\n",
    "\n",
    "# Extracting the results\n",
    "result_columns = [f'{column}_processed' for column in columns[:-1]]\n",
    "processed_df = train_dataset.select(*result_columns)\n",
    "\n",
    "# Aggregating the results\n",
    "aggregated_df = processed_df.agg(\n",
    "    *[spark_sum(col(f'{column}_processed.hash_count')).alias(f'{column}_hash_count') for column in columns[:-1]]\n",
    ")\n",
    "\n",
    "# Show results\n",
    "aggregated_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9daf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate total hashes\n",
    "total_hash = aggregated_df.select(spark_sum(*[col(f'{column}_hash_count') for column in columns[:-1]])).collect()[0][0]\n",
    "print(f\"Number of total hashes: {total_hash}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaspore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
