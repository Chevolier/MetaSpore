{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e8ba8f",
   "metadata": {},
   "source": [
    "# Tutorial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfef096-b2ed-423d-9956-2cb6f8f18988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# data_path = \"data/train/day_0_0.001_train.csv\"\n",
    "\n",
    "# df = pd.read_csv(data_path, header=None)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a3c62-cbd0-4cfe-9888-e84ae4c8ae7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example = df.loc[1, 0]\n",
    "# example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b3fef-cca8-4543-9926-c70bc5bfbfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features = example.split('\\t')\n",
    "# len(features)\n",
    "# features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045523d-1d6e-45a5-8578-7c74cbd561e4",
   "metadata": {},
   "source": [
    "# Read s3 orc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b11f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_11001', '_11002', '_11003', '_11004', '_11007', '_11008', '_11021', '_11022', '_11023', '_11024', '_11041', '_11042', '_11043', '_11044', '_11045', '_11046', '_11061', '_11062', '_11063', '_11064', '_11065', '_11066', '_11081', '_11082', '_11083', '_11084', '_11085', '_11086', '_11601', '_11602', '_11603', '_12001', '_12002', '_12003', '_12004', '_12005', '_12006', '_20001', '_20002', '_20003', '_20101', '_20102', '_20201', '_20202', '_20203', '_20204', '_20205', '_20206', '_20207', '_20208', '_20209', '_20210', '_30001', '_30002', '_30003', '_30004', '_30005', '_30006', '_30201', '_30202', '_30203', '_30204', '_30205', '_30206', '_30207', '_40001', '_40002', '_40003', '_40004', '_40005', '_40201', '_40202', '_40203', '_40204', '_40205', '_40206', '_40207', '_40208', '_40209', '_40210', '_40211', '_40212', '_40213', '_40214', '_40215', '_40231', '_40301', '_40302', '_40303', '_40304', '_40305', '_40306', '_40307', '_40321', '_40322', '_40323', '_40324', '_50801', '_50802', '_50805', '_50806', '_50807', '_50810', '_51001', '_51002', '_51003', '_51004', '_51005', '_51006', '_51011', '_51012', '_51013', '_51014', '_51021', '_51022', '_51023', '_51024', '_51025', '_51026', '_51031', '_51032', '_51033', '_51034', '_51035', '_51036', '_51041', '_51042', '_51043', '_51044', '_51045', '_51046', '_52001', '_52002', '_52003', '_52004', '_52005', '_61101', '_61102', '_61103', '_70001', '_70002', '_70003', '_70004', '_70005', '_70006', '_70007', '_70008', '_70009', '_70010', '_70011', '_70031', '_70032', '_70033', '_70034', '_70035', '_70036', '_70037', '_70038', '_70039', '_70040', '_70041', '_70301', '_70302', '_70303', '_70304', '_70305', '_70306', '_70601', '_70602', '_70603', '_70604', '_70605', '_70606', '_70901', '_70902', '_70903', '_70904', '_70905', '_70906', '_72401', '_72402', '_80001', '_80002', '_80003', '_80004', '_80005', '_80006', '_80007', '_80008', '_80009', '_80010', '_80011', '_80012', '_80013', '_80014', '_80015', '_80016', '_80017', '_80018', '_80019', '_80020', '_80021', '_80022', '_80023', '_80024', '_80025', '_80026', '_80027', '_80028', '_80029', '_80030', '_80031', '_80032', '_80033', '_80034', '_80035', '_80036', '_80037', '_80038', '_80039', '_80040', '_80041', '_80042', '_80043', '_80044', '_80045', '_80046', '_80047', '_80048', '_80049', '_80050', '_80051', '_80052', '_80053', '_80054', '_80055', '_80056', '_80057', '_80058', '_80059', '_80060', '_80061', '_80062', '_80063', '_80064', '_80065', '_80066', '_80067', '_80068', '_80069', '_80070', '_80071', '_80072', '_80073', '_80074', '_80075', '_80076', '_80077', '_80078', '_80079', '_80080', '_80081', '_80082', '_80083', '_80084', '_80085', '_80086', '_80087', '_80088', '_80089', '_80090', '_80091', '_80092', '_80093', '_80094', '_80095', '_80096', '_80097', '_80098', '_80099', '_80100', '_80101', '_80102', '_80103', '_80104', '_80105', '_80106', '_80107', '_80108', '_80109', '_80110', '_80111', '_80112', '_80113', '_80114', '_80115', '_80125', '_80126', '_80127', '_80128', '_80129', '_80130', '_80131', '_80132', '_80133', '_80134', '_80135', '_80136', '_80137', '_80138', '_80139', '_80140', '_80141', '_80142', '_80143', '_80144', '_80145', '_80146', '_80147', '_80148', '_80149', '_80150', '_80151', 'label']\n"
     ]
    }
   ],
   "source": [
    "column_names = []\n",
    "with open(f'./schema/column_name_mobivista.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        column_names.append(line.split(' ')[1].strip())\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b872c64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=====================================>                    (9 + 1) / 14]\r"
     ]
    }
   ],
   "source": [
    "import metaspore as ms\n",
    "\n",
    "# train_dataset_path = ROOT_DIR + '/data/train/day_0_0.001_train.csv'\n",
    "\n",
    "file_base_path = 's3://mv-mtg-di-for-poc-datalab/2024/06/14/00/'\n",
    "num_files = 10\n",
    "file_names = [f'part-{str(i).zfill(5)}-1e73cc51-9b17-4439-9d71-7d505df2cae3-c000.snappy.orc' for i in range(num_files)]\n",
    "train_dataset_path = [file_base_path + file_name for file_name in file_names]\n",
    "\n",
    "# # train_dataset_path = 's3://mv-mtg-di-for-poc-datalab/2024/06/14/00/part-00000-1e73cc51-9b17-4439-9d71-7d505df2cae3-c000.snappy.orc'\n",
    "# train_dataset_path = 's3://mv-mtg-di-for-poc-datalab/2024/06/14/00/'\n",
    "# train_dataset_path = [\n",
    "#     's3://mv-mtg-di-for-poc-datalab/2024/06/14/00/',\n",
    "#     's3://mv-mtg-di-for-poc-datalab/2024/06/14/01/',\n",
    "#     's3://mv-mtg-di-for-poc-datalab/2024/06/14/02/',\n",
    "# ]\n",
    "\n",
    "# train_dataset_path = ['/home/ubuntu/data/processed_parquets/part-00000-03f00bdf-801e-4399-8604-d26e898a24f5-c000.snappy.parquet']\n",
    "\n",
    "spark_confs = {\n",
    "    'spark.eventLog.enabled':'true',\n",
    "    'spark.executor.memory': '10g',\n",
    "    'spark.driver.memory': '10g',\n",
    "    \"spark.driver.maxResultSize\": \"10g\",\n",
    "    \"spark.sql.files.ignoreCorruptFiles\": \"true\"\n",
    "}\n",
    "\n",
    "spark_session = ms.spark.get_session(local=True,\n",
    "                                     batch_size=100,\n",
    "                                     worker_count=1,\n",
    "                                     server_count=0,\n",
    "                                     log_level='WARN',\n",
    "                                     spark_confs=spark_confs)\n",
    "\n",
    "\n",
    "train_dataset = ms.input.read_s3_csv(spark_session, \n",
    "                                    train_dataset_path, \n",
    "                                    format='orc',\n",
    "                                    shuffle=False,\n",
    "                                    delimiter='\\t',\n",
    "                                    multivalue_delimiter=\"\\001\", \n",
    "                                    column_names=column_names,\n",
    "                                    multivalue_column_names=column_names[:-1])\n",
    "\n",
    "# train_dataset.printSchema()\n",
    "# train_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = train_dataset.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d47cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 160.865 s.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField, StringType, FloatType\n",
    "import numpy as np\n",
    "\n",
    "# def process_udf(iterator):\n",
    "#     for minibatch in iterator:\n",
    "#         minibatch = \n",
    "#         yield minibatch\n",
    "\n",
    "\n",
    "def split_value_weight(minibatch):\n",
    "    start_time = time.time()\n",
    "    def split(items):\n",
    "        values = []\n",
    "        weights = []\n",
    "        for item in items:\n",
    "            if '\\003' in item:\n",
    "                value, weight = item.split('\\003')\n",
    "            else:\n",
    "                value, weight = '', '0'\n",
    "            values.append(value)\n",
    "            weights.append(float(weight))\n",
    "        return values, weights\n",
    "\n",
    "    values_dict = {}\n",
    "    weights_dict = {}\n",
    "\n",
    "    for column in minibatch.columns:\n",
    "        if column == 'label':\n",
    "            continue\n",
    "        values_dict[f'{column}'] = []\n",
    "        weights_dict[f'{column}_weight'] = []\n",
    "        for items in minibatch[column]:\n",
    "            try:\n",
    "                # items is expected to be a numpy array or list\n",
    "                if isinstance(items, (np.ndarray, list)):\n",
    "                    values, weights = split(items)\n",
    "                else:\n",
    "                    values, weights = split([items])\n",
    "                values_dict[f'{column}'].append(values)\n",
    "                weights_dict[f'{column}_weight'].append(weights)\n",
    "            except Exception as e:\n",
    "                print(f\"split_value_weight error, {e}, items: {items}\")\n",
    "                values_dict[f'{column}'].append([])\n",
    "                weights_dict[f'{column}_weight'].append([])\n",
    "\n",
    "    minibatch_value = pd.DataFrame(values_dict)\n",
    "    minibatch_weight = pd.DataFrame(weights_dict)\n",
    "\n",
    "    end_time = time.time()\n",
    "    split_duration = end_time - start_time\n",
    "    # print(f\"Type of minibatch: {type(minibatch)}, {minibatch.shape}, split duration: {split_duration:.3f} s.\")\n",
    "\n",
    "    return pd.concat([minibatch_value, minibatch_weight, minibatch[['label']]], axis=1)\n",
    "\n",
    "\n",
    "def process_minibatch(iterator):\n",
    "    for minibatch in iterator:\n",
    "        yield minibatch\n",
    "        # yield split_value_weight(minibatch)\n",
    "\n",
    "def generate_output_schema(columns):\n",
    "    schema_fields = []\n",
    "    for column in columns:\n",
    "        if column == 'label':\n",
    "            continue\n",
    "        schema_fields.append(StructField(f'{column}', ArrayType(StringType()), True))\n",
    "        schema_fields.append(StructField(f'{column}_weight', ArrayType(FloatType()), True))\n",
    "\n",
    "    schema_fields.append(StructField('label', StringType(), True))\n",
    "    return StructType(schema_fields)\n",
    "\n",
    "start_time = time.time()\n",
    "# output_schema = generate_output_schema(train_dataset.columns)\n",
    "output_schema = train_dataset.schema\n",
    "df = train_dataset.mapInPandas(process_minibatch, output_schema)\n",
    "df.collect()\n",
    "duration = time.time() - start_time\n",
    "print(f\"Duration: {duration:.3f} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ba211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadParquetData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Parquet files\n",
    "parquet_path = '/home/ubuntu/data/processed_parquets/part-00000-03f00bdf-801e-4399-8604-d26e898a24f5-c000.snappy.parquet'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.format(\"parquet\").load(parquet_path)\n",
    "\n",
    "# Show the DataFrame schema and content\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ba829",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_weight = [f'{column}_weight' for column in column_names if column!='label']\n",
    "df_weights = pd_df[columns_weight]\n",
    "df_weights.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_weights = df_weights[:1000].to_numpy()\n",
    "flat_weights = np.hstack(np_weights.ravel())\n",
    "flat_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891654f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd_df[['_11001']]\n",
    "print(type(a))\n",
    "a.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = 1\n",
    "column_name = '_11001_weight'\n",
    "\n",
    "# Use take() to retrieve the specific row\n",
    "row = df.select(column_name).take(row_index + 1)[-1]  # take(row_index + 1) gets row 10\n",
    "value = row[column_name]\n",
    "\n",
    "print(f\"Value at column '{column_name}', row {row_index}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d07b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "columns = column_names[:-1]\n",
    "feature_hash = {}\n",
    "feature_value = {}\n",
    "\n",
    "num_files_per_chunk = 10\n",
    "num_chunks = (num_files + num_files_per_chunk - 1) // num_files_per_chunk\n",
    "\n",
    "num_rows = 0\n",
    "\n",
    "for i in tqdm(range(num_chunks), total=num_chunks):\n",
    "    chunk_path = train_dataset_path[i*num_files_per_chunk:(i+1)*num_files_per_chunk]\n",
    "\n",
    "    # Read the chunk\n",
    "    train_dataset = ms.input.read_s3_csv(spark_session, \n",
    "                                     chunk_path, \n",
    "                                     format='orc',\n",
    "                                     shuffle=False,\n",
    "                                     delimiter='\\t', \n",
    "                                     multivalue_delimiter=\"\\001\", \n",
    "                                     column_names=column_names,\n",
    "                                     multivalue_column_names=column_names[:-1])\n",
    "    \n",
    "    # Convert the chunk to a Pandas DataFrame\n",
    "    df = train_dataset.toPandas()\n",
    "    # df.head()\n",
    "    num_rows += df.shape[0]\n",
    "\n",
    "    # num_positive = df[df['label']=='1'].shape[0]\n",
    "    # num_negative = df[df['label']=='0'].shape[0]\n",
    "    # print(f\"num_positive: {num_positive}, num_negative: {num_negative}, ratio: {num_positive/num_negative*100}\")\n",
    "\n",
    "    for i, row in df.iterrows(): # tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        for column in columns[:-1]:\n",
    "            if column in feature_value:\n",
    "                feature_value[column].append(row[column].shape[0])\n",
    "            else:\n",
    "                feature_value[column] = [row[column].shape[0]]\n",
    "            \n",
    "            for item in row[column]:\n",
    "                if '\\003' in item:\n",
    "                    hash, weight = item.split('\\003')\n",
    "                else:\n",
    "                    hash, weight = '', 0\n",
    "                if column in feature_hash:\n",
    "                    feature_hash[column].add(hash)\n",
    "                else:\n",
    "                    feature_hash[column] = set(hash)\n",
    "\n",
    "print(f\"Number of files: {num_files}, number of rows: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = pd.DataFrame(feature_value)\n",
    "df_feature.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_hash_count = dict([(k, [len(v)]) for k, v in feature_hash.items()])\n",
    "df_hash = pd.DataFrame(feature_hash_count)\n",
    "df_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9659a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = set()\n",
    "\n",
    "for k, v in feature_hash.items():\n",
    "    hashes |= v\n",
    "print(f\"Number of total hashes: {len(hashes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hash = df_hash.sum(axis=1).values[0]\n",
    "print(f\"Number of total hashes: {total_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47e508",
   "metadata": {},
   "source": [
    "# Use pyspark to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08cbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "with open(f'./schema/column_name_mobivista.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        columns.append(line.split(' ')[1].strip())\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af234fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaspore as ms\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, explode, sum as spark_sum, countDistinct\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField, StringType\n",
    "\n",
    "# Read column names from file\n",
    "columns = []\n",
    "with open('./schema/column_name_mobivista.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        columns.append(line.split(' ')[1].strip())\n",
    "print(columns)\n",
    "\n",
    "# Define Spark configuration and session\n",
    "file_base_path = 's3://mv-mtg-di-for-poc-datalab/2024/06/14/00/'\n",
    "num_files = 1\n",
    "file_names = [f'part-{str(i).zfill(5)}-1e73cc51-9b17-4439-9d71-7d505df2cae3-c000.snappy.orc' for i in range(num_files)]\n",
    "train_dataset_path = [file_base_path + file_name for file_name in file_names]\n",
    "\n",
    "spark_confs = {\n",
    "    'spark.eventLog.enabled': 'true',\n",
    "    'spark.executor.memory': '10g',\n",
    "    'spark.driver.memory': '10g',\n",
    "    \"spark.driver.maxResultSize\": \"10g\",\n",
    "    \"spark.sql.files.ignoreCorruptFiles\": \"true\"\n",
    "}\n",
    "\n",
    "spark_session = ms.spark.get_session(local=True,\n",
    "                                     batch_size=100,\n",
    "                                     worker_count=3,\n",
    "                                     server_count=3,\n",
    "                                     log_level='WARN',\n",
    "                                     spark_confs=spark_confs)\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = ms.input.read_s3_csv(spark_session, \n",
    "                                     train_dataset_path, \n",
    "                                     format='orc',\n",
    "                                     shuffle=False,\n",
    "                                     delimiter='\\t', \n",
    "                                     multivalue_delimiter=\"\\001\", \n",
    "                                     column_names=columns,\n",
    "                                     multivalue_column_names=columns[:-1])\n",
    "\n",
    "print(f\"Number of orcs: {num_files}, total number of rows: {train_dataset.count()}\")\n",
    "\n",
    "# Define UDF to process rows\n",
    "def process_row(column_values):\n",
    "    feature_hashes = set()\n",
    "    for value in column_values:\n",
    "        if '\\003' in value:\n",
    "            hash_val, weight = value.split('\\003')\n",
    "        else:\n",
    "            hash_val, weight = '', 0\n",
    "        feature_hashes.add(hash_val)\n",
    "\n",
    "    feature_values = len(column_values)\n",
    "    return (feature_values, list(feature_hashes))\n",
    "\n",
    "# Define schema for UDF output\n",
    "schema = StructType([\n",
    "    StructField(\"feature_values\", IntegerType(), True),\n",
    "    StructField(\"hashes\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "# Register the UDF\n",
    "process_row_udf = udf(process_row, schema)\n",
    "\n",
    "# Apply UDF on each column and process the dataset\n",
    "for column in columns[:-1]:\n",
    "    train_dataset = train_dataset.withColumn(f'{column}_processed', process_row_udf(col(column)))\n",
    "\n",
    "train_dataset.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the processed columns\n",
    "result_columns = [f'{column}_processed' for column in columns[:-1]]\n",
    "processed_df = train_dataset.select(*result_columns)\n",
    "processed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the desired row and column\n",
    "row_value = train_dataset.select('_11002_processed').collect()[4][0]\n",
    "\n",
    "# Print the value\n",
    "print(row_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a949529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Obtain the percentiles of feature_values using approxQuantile\n",
    "from tqdm import tqdm\n",
    "percentiles = [0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for col_name in tqdm(processed_df.columns, total=len(processed_df.columns)):\n",
    "    feature_values_df = processed_df.select(col(f\"{col_name}.feature_values\").cast(\"double\").alias(\"feature_values\"))\n",
    "    \n",
    "    # Print schema to verify cast\n",
    "    # feature_values_df.printSchema()\n",
    "    \n",
    "    # Show data to verify cast\n",
    "    # feature_values_df.show()\n",
    "    \n",
    "    # Verify the data type of the column\n",
    "    # data_type = feature_values_df.schema[\"feature_values\"].dataType\n",
    "    # print(f\"Data type of feature_values in {col_name}: {data_type}\")\n",
    "    \n",
    "    # # Check for null values or any anomalies in the column\n",
    "    # null_count = feature_values_df.filter(col(\"feature_values\").isNull()).count()\n",
    "    # print(f\"Number of null values in feature_values of {col_name}: {null_count}\")\n",
    "    \n",
    "    # Obtain approximate percentiles\n",
    "    try:\n",
    "        percentile_values = feature_values_df.approxQuantile(\"feature_values\", percentiles, 0.01)\n",
    "        # print(f\"Percentiles for {col_name}: {percentile_values}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while calculating percentiles for {col_name}: {e}\")\n",
    "\n",
    "# 2. Obtain the total number of different hash values for each column and across all columns\n",
    "distinct_hash_counts = {}\n",
    "all_hashes_df = []\n",
    "\n",
    "print(f\"Starting counting hashes\")\n",
    "for col_name in tqdm(processed_df.columns, total=len(processed_df.columns)):\n",
    "    hashes_df = processed_df.select(explode(col(f\"{col_name}.hashes\")).alias(\"hashes\"))\n",
    "    distinct_count = hashes_df.select(countDistinct(\"hashes\")).first()[0]\n",
    "    distinct_hash_counts[col_name] = distinct_count\n",
    "    all_hashes_df.append(hashes_df)\n",
    "\n",
    "union_hashes_df = all_hashes_df[0]\n",
    "for df in all_hashes_df[1:]:\n",
    "    union_hashes_df = union_hashes_df.union(df)\n",
    "\n",
    "total_distinct_hashes = union_hashes_df.select(countDistinct(\"hashes\")).first()[0]\n",
    "\n",
    "print(f\"Distinct hash counts per column: {distinct_hash_counts}\")\n",
    "print(f\"Total distinct hashes across all columns: {total_distinct_hashes}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d81f09",
   "metadata": {},
   "source": [
    "# To do data approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258919fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Given data\n",
    "number_of_files = np.array([1, 10, 258416]).reshape(-1, 1)\n",
    "number_of_hashes = np.array([1959220, 9758165, 10937500000])\n",
    "\n",
    "# Logarithm fit for the given data\n",
    "x_log = np.log10(number_of_files)\n",
    "y_log = np.log10(number_of_hashes)\n",
    "\n",
    "# Fit the linear model on the log-transformed data\n",
    "log_model = LinearRegression()\n",
    "log_model.fit(x_log, y_log)\n",
    "\n",
    "# Predict missing values on the log scale\n",
    "missing_files = np.array([2148, 35787]).reshape(-1, 1)\n",
    "missing_files_log = np.log10(missing_files)\n",
    "predicted_hashes_log = log_model.predict(missing_files_log)\n",
    "predicted_hashes = 10 ** predicted_hashes_log  # convert back to the original scale\n",
    "\n",
    "# Print predicted values\n",
    "predicted_hashes_dict = {n_files: int(n_hashes) for n_files, n_hashes in zip(missing_files.flatten(), predicted_hashes)}\n",
    "print(predicted_hashes_dict)\n",
    "\n",
    "# Plotting the fit and the predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(number_of_files, number_of_hashes, color='blue', label='Given Data')\n",
    "plt.scatter(missing_files, predicted_hashes, color='red', label='Predicted Data')\n",
    "plt.plot(np.concatenate([number_of_files, missing_files]), \n",
    "         10 ** log_model.predict(np.log10(np.concatenate([number_of_files, missing_files]))), \n",
    "         color='green', label='Logarithmic Fit')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of files')\n",
    "plt.ylabel('Number of hashes')\n",
    "plt.legend()\n",
    "plt.title('Logarithmic Fit for Number of Files vs Number of Hashes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f89e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "2788499930*16*4/10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101cc145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaspore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
