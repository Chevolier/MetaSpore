{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "absolute-birth",
   "metadata": {},
   "source": [
    "# MetaSpore Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-technique",
   "metadata": {
    "tags": []
   },
   "source": [
    "MetaSpore is a machine learning platform, which provides a one-stop solution for data preprocessing, model training and online prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-radiation",
   "metadata": {},
   "source": [
    "In this article, we introduce the basic API of MetaSpore briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-bullet",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-coordinator",
   "metadata": {},
   "source": [
    "We use the publicly available dataset [Terabyte Click Logs](https://labs.criteo.com/2013/12/download-terabyte-click-logs-2/) published by CriteoLabs as our demo dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-setup",
   "metadata": {},
   "source": [
    "We sample the dataset with sampling rate 0.001 so that the running of the demo can finish quickly. More information about the demo dataset can be found in [MetaSpore Demo Dataset](https://ks3-cn-beijing.ksyuncs.com/dmetasoul-bucket/demo/criteo/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-sheep",
   "metadata": {},
   "source": [
    "Execute the following cell to download the demo dataset into the working directory. Those data files take up about 2.1 GiB disk space and the downloading process may take sveral minutes. If the downloading fails, please refer to [MetaSpore Demo Dataset](https://ks3-cn-beijing.ksyuncs.com/dmetasoul-bucket/demo/criteo/index.html) and download the dataset manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "turkish-lebanon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaSpore demo dataset already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for StringBKDRHashFunctionOption not exposed\n",
      "  warnings.warn(\"Python binding for {} not exposed\"\n",
      "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for BKDRHashCombineFunctionOption not exposed\n",
      "  warnings.warn(\"Python binding for {} not exposed\"\n"
     ]
    }
   ],
   "source": [
    "import metaspore\n",
    "metaspore.demo.download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-preference",
   "metadata": {},
   "source": [
    "You can check the downloaded dataset by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inclusive-cover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Jul  6 03:32 test\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Jul  6 03:30 train\n"
     ]
    }
   ],
   "source": [
    "!ls -l ${PWD}/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-plymouth",
   "metadata": {},
   "source": [
    "(Optional) To upload the dataset to your own s3 bucket:\n",
    "\n",
    "1. Fill ``{YOUR_S3_BUCKET}`` and ``{YOUR_S3_PATH}`` with your preferred values in the following cell.\n",
    "2. Uncomment the cell by removing the leading ``#`` character.\n",
    "3. Execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tired-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp --recursive ${PWD}/data/ s3://{YOUR_S3_BUCKET}/{YOUR_S3_PATH}/demo/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-programmer",
   "metadata": {},
   "source": [
    "Alternatively, you can open a terminal by selecting the ``File`` -> ``New`` -> ``Terminal`` menu item and executing Bash commands in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-guide",
   "metadata": {},
   "source": [
    "You can check the uploaded dataset in your s3 bucket by uncommenting and executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "little-password",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!aws s3 ls s3://{YOUR_S3_BUCKET}/{YOUR_S3_PATH}/demo/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33805b-a786-4092-b280-e60a84308672",
   "metadata": {},
   "source": [
    "The ``schema`` directory contains configuration files and must also be uploaded to s3 so that the model can be trained in cluster environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82290030-1bf5-4e9a-8407-9972466c5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp --recursive ${PWD}/schema/ s3://{YOUR_S3_BUCKET}/{YOUR_S3_PATH}/demo/schema/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccbf941-347c-44a4-98b1-c0b2c831dd56",
   "metadata": {},
   "source": [
    "In the rest of the article, we assume the demo dataset and schemas has been uploaded to `ROOT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbae0f69-e2e5-4bae-bcb2-dd6c74016a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_DIR = 's3://{YOUR_S3_BUCKET}/{YOUR_S3_PATH}/demo'\n",
    "ROOT_DIR = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-trustee",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-sacramento",
   "metadata": {},
   "source": [
    "We can define our neural network model by subclassing ``torch.nn.Module`` as usual PyTorch models. The following ``DemoModule`` class provides an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-insight",
   "metadata": {},
   "source": [
    "Compared to usual PyTorch models, the notable difference is the ``_sparse`` layer created by instantiating ``ms.EmbeddingSumConcat`` which takes an embedding size and paths of two text files. ``ms.EmbeddingSumConcat`` makes it possible to define large-scale sparse models in PyTorch, which is a distinguishing feature of MetaSpore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f30e2a-2fc3-4781-9927-300d74d260e2",
   "metadata": {},
   "source": [
    "The ``_schema_dir`` field is an s3 directory which makes it possible to use the ``DemoModule`` class in cluster environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "korean-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import metaspore as ms\n",
    "\n",
    "class DemoModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._embedding_size = 16\n",
    "        self._schema_dir = ROOT_DIR + '/schema/'\n",
    "        self._column_name_path = self._schema_dir + 'column_name_demo.txt'\n",
    "        self._combine_schema_path = self._schema_dir + 'combine_schema_demo.txt'\n",
    "        self._sparse = ms.EmbeddingSumConcat(self._embedding_size, self._column_name_path, self._combine_schema_path)\n",
    "        self._sparse.updater = ms.FTRLTensorUpdater()\n",
    "        self._sparse.initializer = ms.NormalTensorInitializer(var=0.01)\n",
    "        self._dense = torch.nn.Sequential(\n",
    "            ms.nn.Normalization(self._sparse.feature_count * self._embedding_size),\n",
    "            torch.nn.Linear(self._sparse.feature_count * self._embedding_size, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._sparse(x)\n",
    "        x = self._dense(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-vampire",
   "metadata": {},
   "source": [
    "Instantiating the ``DemoModule`` class to define our PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "resistant-justice",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mloaded combine schema from\u001b[m \u001b[32mcombine schema file \u001b[m'./schema/combine_schema_demo.txt'[2024-07-06 03:34:31.240] [info] [local_filesys.cpp:116] Opening local file ./schema/combine_schema_demo.txt with mode r\n",
      "\n",
      "integer_feature_1\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_1, StringBKDRHashFunctionOption::name=integer_feature_1)\n",
      "integer_feature_2\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_2, StringBKDRHashFunctionOption::name=integer_feature_2)\n",
      "integer_feature_3\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_3, StringBKDRHashFunctionOption::name=integer_feature_3)\n",
      "integer_feature_4\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_4, StringBKDRHashFunctionOption::name=integer_feature_4)\n",
      "integer_feature_5\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_5, StringBKDRHashFunctionOption::name=integer_feature_5)\n",
      "integer_feature_6\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_6, StringBKDRHashFunctionOption::name=integer_feature_6)\n",
      "integer_feature_7\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_7, StringBKDRHashFunctionOption::name=integer_feature_7)\n",
      "integer_feature_8\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_8, StringBKDRHashFunctionOption::name=integer_feature_8)\n",
      "integer_feature_9\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_9, StringBKDRHashFunctionOption::name=integer_feature_9)\n",
      "integer_feature_10\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_10, StringBKDRHashFunctionOption::name=integer_feature_10)\n",
      "integer_feature_11\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_11, StringBKDRHashFunctionOption::name=integer_feature_11)\n",
      "integer_feature_12\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_12, StringBKDRHashFunctionOption::name=integer_feature_12)\n",
      "integer_feature_13\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(integer_feature_13, StringBKDRHashFunctionOption::name=integer_feature_13)\n",
      "categorical_feature_1\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_1, StringBKDRHashFunctionOption::name=categorical_feature_1)\n",
      "categorical_feature_2\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_2, StringBKDRHashFunctionOption::name=categorical_feature_2)\n",
      "categorical_feature_3\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_3, StringBKDRHashFunctionOption::name=categorical_feature_3)\n",
      "categorical_feature_4\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_4, StringBKDRHashFunctionOption::name=categorical_feature_4)\n",
      "categorical_feature_5\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_5, StringBKDRHashFunctionOption::name=categorical_feature_5)\n",
      "categorical_feature_6\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_6, StringBKDRHashFunctionOption::name=categorical_feature_6)\n",
      "categorical_feature_7\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_7, StringBKDRHashFunctionOption::name=categorical_feature_7)\n",
      "categorical_feature_8\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_8, StringBKDRHashFunctionOption::name=categorical_feature_8)\n",
      "categorical_feature_9\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_9, StringBKDRHashFunctionOption::name=categorical_feature_9)\n",
      "categorical_feature_10\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_10, StringBKDRHashFunctionOption::name=categorical_feature_10)\n",
      "categorical_feature_11\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_11, StringBKDRHashFunctionOption::name=categorical_feature_11)\n",
      "categorical_feature_12\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_12, StringBKDRHashFunctionOption::name=categorical_feature_12)\n",
      "categorical_feature_13\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_13, StringBKDRHashFunctionOption::name=categorical_feature_13)\n",
      "categorical_feature_14\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_14, StringBKDRHashFunctionOption::name=categorical_feature_14)\n",
      "categorical_feature_15\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_15, StringBKDRHashFunctionOption::name=categorical_feature_15)\n",
      "categorical_feature_16\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_16, StringBKDRHashFunctionOption::name=categorical_feature_16)\n",
      "categorical_feature_17\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_17, StringBKDRHashFunctionOption::name=categorical_feature_17)\n",
      "categorical_feature_18\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_18, StringBKDRHashFunctionOption::name=categorical_feature_18)\n",
      "categorical_feature_19\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_19, StringBKDRHashFunctionOption::name=categorical_feature_19)\n",
      "categorical_feature_20\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_20, StringBKDRHashFunctionOption::name=categorical_feature_20)\n",
      "categorical_feature_21\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_21, StringBKDRHashFunctionOption::name=categorical_feature_21)\n",
      "categorical_feature_22\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_22, StringBKDRHashFunctionOption::name=categorical_feature_22)\n",
      "categorical_feature_23\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_23, StringBKDRHashFunctionOption::name=categorical_feature_23)\n",
      "categorical_feature_24\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_24, StringBKDRHashFunctionOption::name=categorical_feature_24)\n",
      "categorical_feature_25\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_25, StringBKDRHashFunctionOption::name=categorical_feature_25)\n",
      "categorical_feature_26\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash(categorical_feature_26, StringBKDRHashFunctionOption::name=categorical_feature_26)\n",
      "integer_feature_1#categorical_feature_2\n",
      "[2024-07-06 03:34:31.240] [info] add expr bkdr_hash_combine(bkdr_hash(integer_feature_1, StringBKDRHashFunctionOption::name=integer_feature_1), bkdr_hash(categorical_feature_2, StringBKDRHashFunctionOption::name=categorical_feature_2), BKDRHashCombineFunctionOption)\n",
      "integer_feature_5#categorical_feature_10#categorical_feature_5\n",
      "[2024-07-06 03:34:31.241] [info] add expr bkdr_hash_combine(bkdr_hash(integer_feature_5, StringBKDRHashFunctionOption::name=integer_feature_5), bkdr_hash(categorical_feature_10, StringBKDRHashFunctionOption::name=categorical_feature_10), bkdr_hash(categorical_feature_5, StringBKDRHashFunctionOption::name=categorical_feature_5), BKDRHashCombineFunctionOption)\n"
     ]
    }
   ],
   "source": [
    "module = DemoModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-intersection",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-hollow",
   "metadata": {},
   "source": [
    "To train our model, first we need to create a ``ms.PyTorchEstimator`` passing in several arguments including our PyTorch model ``module`` and the number of workers and servers.\n",
    "\n",
    "``model_out_path`` specifies where to store the trained model.\n",
    "\n",
    "``input_label_column_index`` specifies the column index of the label column in the dataset, which is ``0`` for the demo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "matched-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_path = ROOT_DIR + '/output/dev/model_out/'\n",
    "estimator = ms.PyTorchEstimator(module=module,\n",
    "                                worker_count=1,\n",
    "                                server_count=1,\n",
    "                                model_out_path=model_out_path,\n",
    "                                experiment_name='0.1',\n",
    "                                input_label_column_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-hybrid",
   "metadata": {},
   "source": [
    "Next, we create a Spark session by calling ``ms.spark.get_session()`` and load the training dataset by call ``ms.input.read_s3_csv()``.\n",
    "\n",
    "``delimiter`` specifies the column delimiter of the dataset, which is the TAB character ``'\\t'`` for the demo dataset.\n",
    "\n",
    "We also need to pass column names because the csv files do not contain headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a8d6c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'integer_feature_1', 'integer_feature_2', 'integer_feature_3', 'integer_feature_4', 'integer_feature_5', 'integer_feature_6', 'integer_feature_7', 'integer_feature_8', 'integer_feature_9', 'integer_feature_10', 'integer_feature_11', 'integer_feature_12', 'integer_feature_13', 'categorical_feature_1', 'categorical_feature_2', 'categorical_feature_3', 'categorical_feature_4', 'categorical_feature_5', 'categorical_feature_6', 'categorical_feature_7', 'categorical_feature_8', 'categorical_feature_9', 'categorical_feature_10', 'categorical_feature_11', 'categorical_feature_12', 'categorical_feature_13', 'categorical_feature_14', 'categorical_feature_15', 'categorical_feature_16', 'categorical_feature_17', 'categorical_feature_18', 'categorical_feature_19', 'categorical_feature_20', 'categorical_feature_21', 'categorical_feature_22', 'categorical_feature_23', 'categorical_feature_24', 'categorical_feature_25', 'categorical_feature_26']\n"
     ]
    }
   ],
   "source": [
    "column_names = []\n",
    "with open(f'{ROOT_DIR}/schema/column_name_demo.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        column_names.append(line.split(' ')[1].strip())\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "friendly-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/06 03:34:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/06 03:34:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/07/06 03:34:34 INFO SharedState: Warehouse path is 'file:/home/ec2-user/SageMaker/MetaSpore/tutorials/spark-warehouse'.\n",
      "24/07/06 03:34:34 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore shuffle\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path = ROOT_DIR + '/data/train/day_0_0.001_train.csv'\n",
    "\n",
    "spark_session = ms.spark.get_session(local=True,\n",
    "                                     batch_size=100,\n",
    "                                     worker_count=estimator.worker_count,\n",
    "                                     server_count=estimator.server_count,\n",
    "                                     log_level='INFO',\n",
    "                                     spark_confs={'spark.eventLog.enabled':'true'})\n",
    "train_dataset = ms.input.read_s3_csv(spark_session, train_dataset_path, delimiter='\\t', column_names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-hollow",
   "metadata": {},
   "source": [
    "Finally, we call the ``fit()`` method of ``ms.PyTorchEstimator`` to train our model. This will take several minutes and you can see the progress by looking at the output of the cell. The trained model is stored in ``model_out_path`` and the ``model`` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f62c64-41fa-423f-80c7-dc64313bdbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-06 03:34:35.701] [info] PS job with coordinator address 172.16.14.249:47275 started.\n",
      "[2024-07-06 03:34:35.701] [info] PSRunner::RunPS: pid: 31787, tid: 5449, thread: 0x7f10ef598700\n",
      "[2024-07-06 03:34:35.701] [info] PSRunner::RunPSCoordinator: pid: 31787, tid: 5449, thread: 0x7f10ef598700\n",
      "[2024-07-06 03:34:35.702] [info] ActorProcess::Receiving: Coordinator pid: 31787, tid: 5454, thread: 0x7f10e7795700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/06 03:34:35 INFO SparkContext: Starting job: collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:308\n",
      "24/07/06 03:34:35 INFO SparkContext: Starting job: collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:291\n",
      "24/07/06 03:34:35 INFO DAGScheduler: Got job 0 (collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:308) with 1 output partitions\n",
      "24/07/06 03:34:35 INFO DAGScheduler: Final stage: ResultStage 0 (collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:308)\n",
      "24/07/06 03:34:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:35 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:35 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[3] at collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:308), which has no missing parents\n",
      "24/07/06 03:34:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.9 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.9 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[3] at collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:308) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:36 INFO DAGScheduler: Got job 1 (collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:291) with 1 output partitions\n",
      "24/07/06 03:34:36 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:291)\n",
      "24/07/06 03:34:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:36 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:36 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[2] at collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:291), which has no missing parents\n",
      "24/07/06 03:34:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.9 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.9 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[2] at collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:291) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:36 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:36 INFO TaskSchedulerImpl: Registered BarrierCoordinator endpoint\n",
      "24/07/06 03:34:36 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 0.\n",
      "24/07/06 03:34:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:36 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 1.\n",
      "24/07/06 03:34:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/07/06 03:34:36 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for StringBKDRHashFunctionOption not exposed\n",
      "  warnings.warn(\"Python binding for {} not exposed\"\n",
      "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for StringBKDRHashFunctionOption not exposed\n",
      "  warnings.warn(\"Python binding for {} not exposed\"\n",
      "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for BKDRHashCombineFunctionOption not exposed\n",
      "  warnings.warn(\"Python binding for {} not exposed\"\n",
      "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for BKDRHashCombineFunctionOption not exposed\n",
      "  warnings.warn(\"Python binding for {} not exposed\"\n",
      "[2024-07-06 03:34:37.549] [info] PS job with coordinator address 172.16.14.249:47275 started.\n",
      "[2024-07-06 03:34:37.550] [info] PSRunner::RunPS: pid: 5692, tid: 5692, thread: 0x7f4d02bb6740\n",
      "[2024-07-06 03:34:37.550] [info] PSRunner::RunPSServer: pid: 5692, tid: 5692, thread: 0x7f4d02bb6740\n",
      "[2024-07-06 03:34:37.550] [info] PS job with coordinator address 172.16.14.249:47275 started.\n",
      "[2024-07-06 03:34:37.550] [info] PSRunner::RunPS: pid: 5693, tid: 5863, thread: 0x7f4cce0ed700\n",
      "[2024-07-06 03:34:37.550] [info] PSRunner::RunPSWorker: pid: 5693, tid: 5863, thread: 0x7f4cce0ed700\n",
      "\u001b[38;5;046mps agent registered for process 5693 thread 0x7f4d02bb6740\u001b[m\n",
      "[2024-07-06 03:34:37.550] [info] ActorProcess::Receiving: Server pid: 5692, tid: 5868, thread: 0x7f4cccceb700\n",
      "[2024-07-06 03:34:37.551] [info] ActorProcess::Receiving: Worker pid: 5693, tid: 5869, thread: 0x7f4cc7fff700\n",
      "[2024-07-06 03:34:37.551] [info] W[0]:12 has connected to others.\n",
      "[2024-07-06 03:34:37.551] [info] S[0]:10 has connected to others.\n",
      "PS Worker node \u001b[38;5;051mW[0]:12\u001b[m is ready.\n",
      "PS Server node \u001b[38;5;196mS[0]:10\u001b[m is ready.\n",
      "24/07/06 03:34:37 INFO PythonRunner: Times: total = 1352, boot = 418, init = 932, finish = 2\n",
      "24/07/06 03:34:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1366 bytes result sent to driver\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 208.0 B, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1482 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:37 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 49927\n",
      "24/07/06 03:34:37 INFO DAGScheduler: ResultStage 0 (collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:308) finished in 1.601 s\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 MiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.0 MiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 0 finished: collect at /home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py:308, took 1.633175 s\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_2_piece1 stored as bytes in memory (estimated size 617.6 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO BlockManagerInfo: Added broadcast_2_piece1 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 617.6 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO SparkContext: Created broadcast 2 from broadcast at NativeMethodAccessorImpl.java:0\n",
      "24/07/06 03:34:37 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Got job 2 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Final stage: ResultStage 2 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Parents of final stage: List()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-06 03:34:37.551] [info] C[0]:9: The coordinator has connected to 1 servers and 1 workers.\n",
      "PS Coordinator node \u001b[32mC[0]:9\u001b[m is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/06 03:34:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[5] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-16-14-249.us-west-2.compute.internal:39421 in memory (size: 4.9 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.3 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 3.3 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (PythonRDD[5] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 2.\n",
      "24/07/06 03:34:37 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "integer_feature_1\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_1, StringBKDRHashFunctionOption::name=integer_feature_1)\n",
      "integer_feature_2\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_2, StringBKDRHashFunctionOption::name=integer_feature_2)\n",
      "integer_feature_3\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_3, StringBKDRHashFunctionOption::name=integer_feature_3)\n",
      "integer_feature_4\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_4, StringBKDRHashFunctionOption::name=integer_feature_4)\n",
      "integer_feature_5\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_5, StringBKDRHashFunctionOption::name=integer_feature_5)\n",
      "integer_feature_6\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_6, StringBKDRHashFunctionOption::name=integer_feature_6)\n",
      "integer_feature_7\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_7, StringBKDRHashFunctionOption::name=integer_feature_7)\n",
      "integer_feature_8\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_8, StringBKDRHashFunctionOption::name=integer_feature_8)\n",
      "integer_feature_9\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_9, StringBKDRHashFunctionOption::name=integer_feature_9)\n",
      "integer_feature_10\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_10, StringBKDRHashFunctionOption::name=integer_feature_10)\n",
      "integer_feature_11\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_11, StringBKDRHashFunctionOption::name=integer_feature_11)\n",
      "integer_feature_12\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_12, StringBKDRHashFunctionOption::name=integer_feature_12)\n",
      "integer_feature_13\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(integer_feature_13, StringBKDRHashFunctionOption::name=integer_feature_13)\n",
      "categorical_feature_1\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_1, StringBKDRHashFunctionOption::name=categorical_feature_1)\n",
      "categorical_feature_2\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_2, StringBKDRHashFunctionOption::name=categorical_feature_2)\n",
      "categorical_feature_3\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_3, StringBKDRHashFunctionOption::name=categorical_feature_3)\n",
      "categorical_feature_4\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_4, StringBKDRHashFunctionOption::name=categorical_feature_4)\n",
      "categorical_feature_5\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_5, StringBKDRHashFunctionOption::name=categorical_feature_5)\n",
      "categorical_feature_6\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_6, StringBKDRHashFunctionOption::name=categorical_feature_6)\n",
      "categorical_feature_7\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_7, StringBKDRHashFunctionOption::name=categorical_feature_7)\n",
      "categorical_feature_8\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_8, StringBKDRHashFunctionOption::name=categorical_feature_8)\n",
      "categorical_feature_9\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_9, StringBKDRHashFunctionOption::name=categorical_feature_9)\n",
      "categorical_feature_10\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_10, StringBKDRHashFunctionOption::name=categorical_feature_10)\n",
      "categorical_feature_11\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_11, StringBKDRHashFunctionOption::name=categorical_feature_11)\n",
      "categorical_feature_12\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_12, StringBKDRHashFunctionOption::name=categorical_feature_12)\n",
      "categorical_feature_13\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_13, StringBKDRHashFunctionOption::name=categorical_feature_13)\n",
      "categorical_feature_14\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_14, StringBKDRHashFunctionOption::name=categorical_feature_14)\n",
      "categorical_feature_15\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_15, StringBKDRHashFunctionOption::name=categorical_feature_15)\n",
      "categorical_feature_16\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_16, StringBKDRHashFunctionOption::name=categorical_feature_16)\n",
      "categorical_feature_17\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_17, StringBKDRHashFunctionOption::name=categorical_feature_17)\n",
      "categorical_feature_18\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_18, StringBKDRHashFunctionOption::name=categorical_feature_18)\n",
      "categorical_feature_19\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_19, StringBKDRHashFunctionOption::name=categorical_feature_19)\n",
      "categorical_feature_20\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_20, StringBKDRHashFunctionOption::name=categorical_feature_20)\n",
      "categorical_feature_21\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_21, StringBKDRHashFunctionOption::name=categorical_feature_21)\n",
      "categorical_feature_22\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_22, StringBKDRHashFunctionOption::name=categorical_feature_22)\n",
      "categorical_feature_23\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_23, StringBKDRHashFunctionOption::name=categorical_feature_23)\n",
      "categorical_feature_24\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_24, StringBKDRHashFunctionOption::name=categorical_feature_24)\n",
      "categorical_feature_25\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_25, StringBKDRHashFunctionOption::name=categorical_feature_25)\n",
      "categorical_feature_26\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash(categorical_feature_26, StringBKDRHashFunctionOption::name=categorical_feature_26)\n",
      "integer_feature_1#categorical_feature_2\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash_combine(bkdr_hash(integer_feature_1, StringBKDRHashFunctionOption::name=integer_feature_1), bkdr_hash(categorical_feature_2, StringBKDRHashFunctionOption::name=categorical_feature_2), BKDRHashCombineFunctionOption)\n",
      "integer_feature_5#categorical_feature_10#categorical_feature_5\n",
      "[2024-07-06 03:34:37.644] [info] add expr bkdr_hash_combine(bkdr_hash(integer_feature_5, StringBKDRHashFunctionOption::name=integer_feature_5), bkdr_hash(categorical_feature_10, StringBKDRHashFunctionOption::name=categorical_feature_10), bkdr_hash(categorical_feature_5, StringBKDRHashFunctionOption::name=categorical_feature_5), BKDRHashCombineFunctionOption)\n",
      "24/07/06 03:34:37 INFO PythonRunner: Times: total = 11, boot = -28, init = 35, finish = 4\n",
      "24/07/06 03:34:37 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 24 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:37 INFO DAGScheduler: ResultStage 2 (collect at PythonRDD.scala:195) finished in 0.031 s\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 2 finished: collect at PythonRDD.scala:195, took 0.034598 s\n",
      "24/07/06 03:34:37 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Got job 3 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Final stage: ResultStage 3 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.3 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.5 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 3.\n",
      "24/07/06 03:34:37 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "24/07/06 03:34:37 INFO PythonRunner: Times: total = 43, boot = 23, init = 20, finish = 0\n",
      "24/07/06 03:34:37 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 53 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:37 INFO DAGScheduler: ResultStage 3 (collect at PythonRDD.scala:195) finished in 0.060 s\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 3 finished: collect at PythonRDD.scala:195, took 0.063304 s\n",
      "24/07/06 03:34:37 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Got job 4 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Final stage: ResultStage 4 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[9] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.2 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.4 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (PythonRDD[9] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 4.\n",
      "24/07/06 03:34:37 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "24/07/06 03:34:37 INFO PythonRunner: Times: total = 43, boot = 20, init = 23, finish = 0\n",
      "24/07/06 03:34:37 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 53 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:37 INFO DAGScheduler: ResultStage 4 (collect at PythonRDD.scala:195) finished in 0.061 s\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 4 finished: collect at PythonRDD.scala:195, took 0.063933 s\n",
      "24/07/06 03:34:37 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Got job 5 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Final stage: ResultStage 5 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[11] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.2 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.4 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (PythonRDD[11] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 5.\n",
      "24/07/06 03:34:37 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "24/07/06 03:34:37 INFO PythonRunner: Times: total = 42, boot = 19, init = 23, finish = 0\n",
      "24/07/06 03:34:37 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 52 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:37 INFO DAGScheduler: ResultStage 5 (collect at PythonRDD.scala:195) finished in 0.059 s\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 5 finished: collect at PythonRDD.scala:195, took 0.062501 s\n",
      "24/07/06 03:34:37 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Got job 6 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Final stage: ResultStage 6 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[13] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 7.3 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.4 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (PythonRDD[13] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 6.\n",
      "24/07/06 03:34:37 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
      "24/07/06 03:34:37 INFO PythonRunner: Times: total = 42, boot = 22, init = 20, finish = 0\n",
      "24/07/06 03:34:37 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 52 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:37 INFO DAGScheduler: ResultStage 6 (collect at PythonRDD.scala:195) finished in 0.059 s\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Job 6 finished: collect at PythonRDD.scala:195, took 0.062410 s\n",
      "24/07/06 03:34:37 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Got job 7 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Final stage: ResultStage 7 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[15] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.3 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.4 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:37 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (PythonRDD[15] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 7.\n",
      "24/07/06 03:34:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
      "24/07/06 03:34:38 INFO PythonRunner: Times: total = 44, boot = 23, init = 21, finish = 0\n",
      "24/07/06 03:34:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 53 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:38 INFO DAGScheduler: ResultStage 7 (collect at PythonRDD.scala:195) finished in 0.060 s\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 7 finished: collect at PythonRDD.scala:195, took 0.063206 s\n",
      "24/07/06 03:34:38 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Got job 8 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Final stage: ResultStage 8 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting ResultStage 8 (PythonRDD[17] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.3 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.4 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (PythonRDD[17] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 8.\n",
      "24/07/06 03:34:38 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
      "24/07/06 03:34:38 INFO PythonRunner: Times: total = 42, boot = 23, init = 19, finish = 0\n",
      "24/07/06 03:34:38 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 50 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:38 INFO DAGScheduler: ResultStage 8 (collect at PythonRDD.scala:195) finished in 0.057 s\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 8 finished: collect at PythonRDD.scala:195, took 0.059925 s\n",
      "24/07/06 03:34:38 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Got job 9 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Final stage: ResultStage 9 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[19] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.3 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.4 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (PythonRDD[19] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 9.\n",
      "24/07/06 03:34:38 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
      "24/07/06 03:34:38 INFO PythonRunner: Times: total = 50, boot = 20, init = 30, finish = 0\n",
      "24/07/06 03:34:38 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 58 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:38 INFO DAGScheduler: ResultStage 9 (collect at PythonRDD.scala:195) finished in 0.065 s\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 9 finished: collect at PythonRDD.scala:195, took 0.066841 s\n",
      "24/07/06 03:34:38 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Got job 10 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Final stage: ResultStage 10 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[21] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 7.3 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.4 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (PythonRDD[21] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 10.\n",
      "24/07/06 03:34:38 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
      "24/07/06 03:34:38 INFO PythonRunner: Times: total = 43, boot = 23, init = 20, finish = 0\n",
      "24/07/06 03:34:38 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 50 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:38 INFO DAGScheduler: ResultStage 10 (collect at PythonRDD.scala:195) finished in 0.057 s\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 10 finished: collect at PythonRDD.scala:195, took 0.060421 s\n",
      "24/07/06 03:34:38 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Got job 11 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Final stage: ResultStage 11 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[23] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 7.2 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.4 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (PythonRDD[23] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 11.\n",
      "24/07/06 03:34:38 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
      "24/07/06 03:34:38 INFO PythonRunner: Times: total = 44, boot = 24, init = 20, finish = 0\n",
      "24/07/06 03:34:38 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 52 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:38 INFO DAGScheduler: ResultStage 11 (collect at PythonRDD.scala:195) finished in 0.058 s\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 11 finished: collect at PythonRDD.scala:195, took 0.060837 s\n",
      "24/07/06 03:34:38 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Got job 12 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Final stage: ResultStage 12 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting ResultStage 12 (PythonRDD[25] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 7.2 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.4 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (PythonRDD[25] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 12.\n",
      "24/07/06 03:34:38 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)\n",
      "24/07/06 03:34:38 INFO PythonRunner: Times: total = 44, boot = 23, init = 21, finish = 0\n",
      "24/07/06 03:34:38 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 51 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:38 INFO DAGScheduler: ResultStage 12 (collect at PythonRDD.scala:195) finished in 0.058 s\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 12 finished: collect at PythonRDD.scala:195, took 0.059956 s\n",
      "24/07/06 03:34:38 INFO SparkContext: Starting job: collect at PythonRDD.scala:195\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Got job 13 (collect at PythonRDD.scala:195) with 1 output partitions\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Final stage: ResultStage 13 (collect at PythonRDD.scala:195)\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting ResultStage 13 (PythonRDD[27] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 7.2 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 4.4 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:38 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (PythonRDD[27] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 13.\n",
      "24/07/06 03:34:38 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)\n",
      "\u001b[38;5;196minit sparse tensor _sparse with slice shape (16,), updater FTRLTensorUpdater(1.0, 120.0, 0.5, 1.0) and initializer NormalTensorInitializer(0.0, 0.01)\u001b[m\n",
      "\u001b[38;5;046minit dense tensor _dense.0.weight with shape (656, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer OneTensorInitializer()\u001b[m\n",
      "\u001b[38;5;046minit dense tensor _dense.0.bias with shape (656, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer ZeroTensorInitializer()\u001b[m\n",
      "\u001b[38;5;046minit dense tensor _dense.1.weight with shape (1024, 656), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()\u001b[m\n",
      "\u001b[38;5;046minit dense tensor _dense.1.bias with shape (1024, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()\u001b[m\n",
      "\u001b[38;5;046minit dense tensor _dense.3.weight with shape (512, 1024), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()\u001b[m\n",
      "\u001b[38;5;046minit dense tensor _dense.3.bias with shape (512, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()\u001b[m\n",
      "\u001b[38;5;046minit dense tensor _dense.5.weight with shape (1, 512), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()\u001b[m\n",
      "\u001b[38;5;046minit dense tensor _dense.5.bias with shape (1, 1), updater AdamTensorUpdater(1e-05, 0.9, 0.999, 1e-08) and initializer DefaultTensorInitializer()\u001b[m\n",
      "\u001b[38;5;051minit dense tensor _dense.0.running_mean with shape (656, 1), updater EMATensorUpdater(0.1) and initializer ZeroTensorInitializer()\u001b[m\n",
      "\u001b[38;5;051minit dense tensor _dense.0.running_var with shape (656, 1), updater EMATensorUpdater(0.1) and initializer OneTensorInitializer()\u001b[m\n",
      "24/07/06 03:34:38 INFO PythonRunner: Times: total = 78, boot = 23, init = 26, finish = 29\n",
      "24/07/06 03:34:38 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 1323 bytes result sent to driver\n",
      "24/07/06 03:34:38 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 84 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/1)\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:38 INFO DAGScheduler: ResultStage 13 (collect at PythonRDD.scala:195) finished in 0.091 s\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/07/06 03:34:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "24/07/06 03:34:38 INFO DAGScheduler: Job 13 finished: collect at PythonRDD.scala:195, took 0.093345 s\n",
      "24/07/06 03:34:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/07/06 03:34:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/07/06 03:34:39 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/07/06 03:34:39 INFO CodeGenerator: Code generated in 177.60474 ms(0 + 1) / 1]\n",
      "24/07/06 03:34:39 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 212.1 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:39 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 35.6 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:39 INFO SparkContext: Created broadcast 15 from save at NativeMethodAccessorImpl.java:0\n",
      "24/07/06 03:34:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 27018487 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Registering RDD 31 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Got map stage job 14 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[31] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/07/06 03:34:39 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 38.2 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:39 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:39 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 13.0 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:39 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[31] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/07/06 03:34:39 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0\n",
      "24/07/06 03:34:39 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 8251 bytes) \n",
      "24/07/06 03:34:39 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)\n",
      "24/07/06 03:34:39 INFO CodeGenerator: Code generated in 23.98835 ms\n",
      "24/07/06 03:34:39 INFO FileScanRDD: Reading File path: file:///home/ec2-user/SageMaker/MetaSpore/tutorials/data/train/day_0_0.001_train.csv, range: 0-27018487, partition values: [empty row]\n",
      "24/07/06 03:34:39 INFO CodeGenerator: Code generated in 20.244965 ms\n",
      "24/07/06 03:34:39 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 1861 bytes result sent to driver\n",
      "24/07/06 03:34:39 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 15) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 1, PROCESS_LOCAL, 8251 bytes) \n",
      "24/07/06 03:34:39 INFO Executor: Running task 1.0 in stage 14.0 (TID 15)\n",
      "24/07/06 03:34:39 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 205 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (1/2)\n",
      "24/07/06 03:34:39 INFO FileScanRDD: Reading File path: file:///home/ec2-user/SageMaker/MetaSpore/tutorials/data/train/day_0_0.001_train.csv, range: 27018487-49842670, partition values: [empty row]\n",
      "24/07/06 03:34:39 INFO Executor: Finished task 1.0 in stage 14.0 (TID 15). 1818 bytes result sent to driver\n",
      "24/07/06 03:34:39 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 15) in 28 ms on ip-172-16-14-249.us-west-2.compute.internal (executor driver) (2/2)\n",
      "24/07/06 03:34:39 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:39 INFO DAGScheduler: ShuffleMapStage 14 (save at NativeMethodAccessorImpl.java:0) finished in 0.275 s\n",
      "24/07/06 03:34:39 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/07/06 03:34:39 INFO DAGScheduler: running: Set(ResultStage 1)\n",
      "24/07/06 03:34:39 INFO DAGScheduler: waiting: Set()\n",
      "24/07/06 03:34:39 INFO DAGScheduler: failed: Set()\n",
      "24/07/06 03:34:39 INFO CodeGenerator: Code generated in 16.678488 ms\n",
      "24/07/06 03:34:39 INFO OverwriteByExpressionExec: Start processing data source write support: org.apache.spark.sql.execution.datasources.noop.NoopBatchWrite$@851288a. The input RDD has 1 partitions.\n",
      "24/07/06 03:34:39 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Got job 15 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Final stage: ResultStage 16 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[34] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/07/06 03:34:39 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 36.6 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:39 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 11.7 KiB, free 2.8 GiB)\n",
      "24/07/06 03:34:39 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ip-172-16-14-249.us-west-2.compute.internal:39421 (size: 11.7 KiB, free: 2.8 GiB)\n",
      "24/07/06 03:34:39 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585\n",
      "24/07/06 03:34:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[34] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/07/06 03:34:39 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "24/07/06 03:34:39 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (ip-172-16-14-249.us-west-2.compute.internal, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/07/06 03:34:39 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)\n",
      "24/07/06 03:34:39 INFO ShuffleBlockFetcherIterator: Getting 2 (50.1 KiB) non-empty blocks including 2 (50.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/07/06 03:34:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
      "24/07/06 03:34:39 INFO CodeGenerator: Code generated in 14.261218 ms\n",
      "24/07/06 03:34:40 INFO BaseAllocator: Debug mode disabled.\n",
      "24/07/06 03:34:40 INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type\n",
      "24/07/06 03:34:40 INFO CheckAllocator: Using DefaultAllocationManager at memory-netty-12.0.1.jar!/org/apache/arrow/memory/DefaultAllocationManagerFactory.class\n",
      "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for StringBKDRHashFunctionOption not exposed\n",
      "  warnings.warn(\"Python binding for {} not exposed\"\n",
      "/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/pyarrow/compute.py:196: RuntimeWarning: Python binding for BKDRHashCombineFunctionOption not exposed\n",
      "  warnings.warn(\"Python binding for {} not exposed\"\n",
      "24/07/06 03:34:41 ERROR Utils: Aborting task\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py\", line 382, in _feed_training_minibatch\n",
      "    self = __class__.get_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py\", line 160, in get_instance\n",
      "    return instance\n",
      "RuntimeError: no ps agent registered for thread 0x7f9fb86db740 on pid 5964\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "24/07/06 03:34:41 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 16, attempt 0, stage 16.0)\n",
      "24/07/06 03:34:41 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 16, attempt 0, stage 16.0)\n",
      "24/07/06 03:34:41 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 16)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py\", line 382, in _feed_training_minibatch\n",
      "    self = __class__.get_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py\", line 160, in get_instance\n",
      "    return instance\n",
      "RuntimeError: no ps agent registered for thread 0x7f9fb86db740 on pid 5964\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "24/07/06 03:34:41 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 16) (ip-172-16-14-249.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py\", line 382, in _feed_training_minibatch\n",
      "    self = __class__.get_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py\", line 160, in get_instance\n",
      "    return instance\n",
      "RuntimeError: no ps agent registered for thread 0x7f9fb86db740 on pid 5964\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "24/07/06 03:34:41 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job\n",
      "24/07/06 03:34:41 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/07/06 03:34:41 INFO TaskSchedulerImpl: Cancelling stage 16\n",
      "24/07/06 03:34:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 16) (ip-172-16-14-249.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py\", line 382, in _feed_training_minibatch\n",
      "    self = __class__.get_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py\", line 160, in get_instance\n",
      "    return instance\n",
      "RuntimeError: no ps agent registered for thread 0x7f9fb86db740 on pid 5964\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/07/06 03:34:41 INFO DAGScheduler: ResultStage 16 (save at NativeMethodAccessorImpl.java:0) failed in 1.544 s due to Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 16) (ip-172-16-14-249.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py\", line 382, in _feed_training_minibatch\n",
      "    self = __class__.get_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/metaspore/lib/python3.8/site-packages/metaspore/agent.py\", line 160, in get_instance\n",
      "    return instance\n",
      "RuntimeError: no ps agent registered for thread 0x7f9fb86db740 on pid 5964\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/07/06 03:34:41 INFO DAGScheduler: Job 15 failed: save at NativeMethodAccessorImpl.java:0, took 1.555373 s\n",
      "24/07/06 03:34:41 ERROR OverwriteByExpressionExec: Data source write support org.apache.spark.sql.execution.datasources.noop.NoopBatchWrite$@851288a is aborting.\n",
      "24/07/06 03:34:41 ERROR OverwriteByExpressionExec: Data source write support org.apache.spark.sql.execution.datasources.noop.NoopBatchWrite$@851288a aborted.\n"
     ]
    }
   ],
   "source": [
    "model = estimator.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-quest",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-debut",
   "metadata": {},
   "source": [
    "To evaluate our model, we use the ``ms.input.read_s3_csv()`` function again to load the test dataset, passing in the column delimiter ``'\\t'``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = ROOT_DIR + '/data/test/day_0_0.001_test.csv'\n",
    "test_dataset = ms.input.read_s3_csv(spark_session, test_dataset_path, delimiter='\\t', column_names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-timeline",
   "metadata": {},
   "source": [
    "Next, we call the ``model.transform()`` method to transform the test dataset, which will add a column named ``rawPrediction`` to the test dataset representing the predicted labels. For ease of integration with Spark MLlib, ``model.transform()`` will also add a column named ``label`` to the test dataset representing the actual labels.\n",
    "\n",
    "Like the training process, this will take several minutes and you can see the progress by looking at the output of the cell. The transformed test dataset is stored in the ``result`` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-yesterday",
   "metadata": {},
   "source": [
    "``result`` is a normal PySpark DataFrame and can be inspected by its methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-rebate",
   "metadata": {},
   "source": [
    "Finally, we use ``pyspark.ml.evaluation.BinaryClassificationEvaluator`` to compute test AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-football",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "evaluator = pyspark.ml.evaluation.BinaryClassificationEvaluator()\n",
    "test_auc = evaluator.evaluate(result)\n",
    "print('test_auc: %g' % test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256721e7-144d-479a-8fbd-0be3479137a2",
   "metadata": {},
   "source": [
    "When all computations are done, we should call the ``stop()`` method of ``spark_session`` to make sure all the resources are released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb448788-b6ad-466c-8e9d-ff38d5235dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-company",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-outside",
   "metadata": {},
   "source": [
    "We illustrated how to train and evaluate neural network model in MetaSpore. Users familiar with PyTorch and Spark MLlib should get started easily, which is the design goal of MetaSpore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_metaspore",
   "language": "python",
   "name": "conda_metaspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
